{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "MNIST classification with TensorFlow's Dataset API.\n",
    "Introduced in TensorFlow 1.3, the Dataset API is now the\n",
    "standard method for loading data into TensorFlow models.\n",
    "A Dataset is a sequence of elements, which are themselves\n",
    "composed of tf.Tensor components. For more details, see:\n",
    "https://www.tensorflow.org/programmers_guide/datasets\n",
    "To use this with Keras, we make a dataset out of elements\n",
    "of the form (input batch, output batch). From there, we\n",
    "create a one-shot iterator and a graph node corresponding\n",
    "to its get_next() method. Its components are then provided\n",
    "to the network's Input layer and the Model.compile() method,\n",
    "respectively.\n",
    "This example is intended to closely follow the\n",
    "mnist_tfrecord.py example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 197s 421ms/step - loss: 0.1534 - acc: 0.9529\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 80s 171ms/step - loss: 0.0462 - acc: 0.9857\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 86s 183ms/step - loss: 0.0360 - acc: 0.9889\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 83s 176ms/step - loss: 0.0276 - acc: 0.9914\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 82s 175ms/step - loss: 0.0242 - acc: 0.9926\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 6s 628us/step\n",
      "\n",
      "Test accuracy: 0.9887999976277352\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the TensorFlow backend,'\n",
    "                       ' because it requires the Datset API, which is not'\n",
    "                       ' supported on other platforms.')\n",
    "\n",
    "\n",
    "def cnn_layers(inputs):\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    predictions = layers.Dense(num_classes,\n",
    "                               activation='softmax',\n",
    "                               name='x_train_out')(x)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "steps_per_epoch = int(np.ceil(60000 / float(batch_size)))  # = 469\n",
    "epochs = 5\n",
    "num_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = tf.one_hot(y_train, num_classes)\n",
    "\n",
    "# Create the dataset and its associated one-shot iterator.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Model creation using tensors from the get_next() graph node.\n",
    "inputs, targets = iterator.get_next()\n",
    "model_input = layers.Input(tensor=inputs)\n",
    "model_output = cnn_layers(model_input)\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    target_tensors=[targets])\n",
    "train_model.summary()\n",
    "\n",
    "train_model.fit(epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the model weights.\n",
    "weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5')\n",
    "train_model.save_weights(weight_path)\n",
    "\n",
    "# Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# Second session to test loading trained model without tensors.\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_test_inp = layers.Input(shape=x_test.shape[1:])\n",
    "test_out = cnn_layers(x_test_inp)\n",
    "test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n",
    "\n",
    "test_model.load_weights(weight_path)\n",
    "test_model.compile(optimizer='rmsprop',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "test_model.summary()\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test, y_test, num_classes)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
