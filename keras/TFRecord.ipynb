{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "MNIST dataset with TFRecords, the standard TensorFlow data format.\n",
    "TFRecord is a data format supported throughout TensorFlow.\n",
    "This example demonstrates how to load TFRecord data using\n",
    "Input Tensors. Input Tensors differ from the normal Keras\n",
    "workflow because instead of fitting to data loaded into a\n",
    "a numpy array, data is supplied via a special tensor that\n",
    "reads data from nodes that are wired directly into model\n",
    "graph with the `Input(tensor=input_tensor)` parameter.\n",
    "There are several advantages to using Input Tensors.\n",
    "First, if a dataset is already in TFRecord format you\n",
    "can load and train on that data directly in Keras.\n",
    "Second, extended backend API capabilities such as TensorFlow\n",
    "data augmentation is easy to integrate directly into your\n",
    "Keras training scripts via input tensors.\n",
    "Third, TensorFlow implements several data APIs for\n",
    "TFRecords, some of which provide significantly faster\n",
    "training performance than numpy arrays can provide because\n",
    "they run via the C++ backend. Please note that this\n",
    "example is tailored for brevity and clarity and not\n",
    "to demonstrate performance or augmentation capabilities.\n",
    "Input Tensors also have important disadvantages. In\n",
    "particular, Input Tensors are fixed at model construction\n",
    "because rewiring networks is not yet supported.\n",
    "For this reason, changing the data input source means\n",
    "model weights must be saved and the model rebuilt\n",
    "from scratch to connect the new input data.\n",
    "validation cannot currently be performed as training\n",
    "progresses, and must be performed after training completes.\n",
    "This example demonstrates how to train with input\n",
    "tensors, save the model weights, and then evaluate the\n",
    "model using the numpy based Keras API.\n",
    "Gets to ~99.1% test accuracy after 5 epochs\n",
    "(high variance from run to run: 98.9-99.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-c886f91758ee>:104: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/zhangyong/.keras/datasets/MNIST-data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/zhangyong/.keras/datasets/MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /Users/zhangyong/.keras/datasets/MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /Users/zhangyong/.keras/datasets/MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (100, 28, 28, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (100, 26, 26, 32)         320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (100, 13, 13, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (100, 11, 11, 64)         18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (100, 5, 5, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 1600)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 512)                819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (100, 512)                0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (100, 10)                 5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 205s 342ms/step - loss: 0.1597 - acc: 0.9517\n",
      "100/100 [==============================] - 6s 62ms/step\n",
      "\n",
      "val_loss: 0.05461542011791607 val_acc: 0.9806000071763993 \n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 99s 165ms/step - loss: 0.0508 - acc: 0.9846\n",
      "100/100 [==============================] - 5s 51ms/step\n",
      "\n",
      "val_loss: 0.027697966343548614 val_acc: 0.9902000063657761 \n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 97s 162ms/step - loss: 0.0373 - acc: 0.9887\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "\n",
      "val_loss: 0.030491687441679006 val_acc: 0.9912000077962876 \n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.0328 - acc: 0.9903\n",
      "100/100 [==============================] - 5s 46ms/step\n",
      "\n",
      "val_loss: 0.03406681943049989 val_acc: 0.9902000057697297 \n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.0273 - acc: 0.9920\n",
      "100/100 [==============================] - 5s 47ms/step\n",
      "\n",
      "val_loss: 0.026686970202326847 val_acc: 0.9924000054597855 \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 5s 478us/step\n",
      "\n",
      "Test accuracy: 0.9924000054597855\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the '\n",
    "                       'TensorFlow backend, '\n",
    "                       'because it requires TFRecords, which '\n",
    "                       'are not supported on other platforms.')\n",
    "\n",
    "\n",
    "class EvaluateInputTensor(Callback):\n",
    "    \"\"\" Validate a model which does not expect external numpy data during training.\n",
    "    Keras does not expect external numpy data at training time, and thus cannot\n",
    "    accept numpy arrays for validation when all of a Keras Model's\n",
    "    `Input(input_tensor)` layers are provided an  `input_tensor` parameter,\n",
    "    and the call to `Model.compile(target_tensors)` defines all `target_tensors`.\n",
    "    Instead, create a second model for validation which is also configured\n",
    "    with input tensors and add it to the `EvaluateInputTensor` callback\n",
    "    to perform validation.\n",
    "    It is recommended that this callback be the first in the list of callbacks\n",
    "    because it defines the validation variables required by many other callbacks,\n",
    "    and Callbacks are made in order.\n",
    "    # Arguments\n",
    "        model: Keras model on which to call model.evaluate().\n",
    "        steps: Integer or `None`.\n",
    "            Total number of steps (batches of samples)\n",
    "            before declaring the evaluation round finished.\n",
    "            Ignored with the default value of `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, steps, metrics_prefix='val', verbose=1):\n",
    "        # parameter of callbacks passed during initialization\n",
    "        # pass evalation mode directly\n",
    "        super(EvaluateInputTensor, self).__init__()\n",
    "        self.val_model = model\n",
    "        self.num_steps = steps\n",
    "        self.verbose = verbose\n",
    "        self.metrics_prefix = metrics_prefix\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.val_model.set_weights(self.model.get_weights())\n",
    "        results = self.val_model.evaluate(None, None, steps=int(self.num_steps),\n",
    "                                          verbose=self.verbose)\n",
    "        metrics_str = '\\n'\n",
    "        for result, name in zip(results, self.val_model.metrics_names):\n",
    "            metric_name = self.metrics_prefix + '_' + name\n",
    "            logs[metric_name] = result\n",
    "            if self.verbose > 0:\n",
    "                metrics_str = metrics_str + metric_name + ': ' + str(result) + ' '\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(metrics_str)\n",
    "\n",
    "\n",
    "def cnn_layers(x_train_input):\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu', padding='valid')(x_train_input)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x_train_out = layers.Dense(num_classes,\n",
    "                               activation='softmax',\n",
    "                               name='x_train_out')(x)\n",
    "    return x_train_out\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "batch_size = 100\n",
    "batch_shape = (batch_size, 28, 28, 1)\n",
    "epochs = 5\n",
    "num_classes = 10\n",
    "\n",
    "# The capacity variable controls the maximum queue size\n",
    "# allowed when prefetching data for training.\n",
    "capacity = 10000\n",
    "\n",
    "# min_after_dequeue is the minimum number elements in the queue\n",
    "# after a dequeue, which ensures sufficient mixing of elements.\n",
    "min_after_dequeue = 3000\n",
    "\n",
    "# If `enqueue_many` is `False`, `tensors` is assumed to represent a\n",
    "# single example.  An input tensor with shape `[x, y, z]` will be output\n",
    "# as a tensor with shape `[batch_size, x, y, z]`.\n",
    "#\n",
    "# If `enqueue_many` is `True`, `tensors` is assumed to represent a\n",
    "# batch of examples, where the first dimension is indexed by example,\n",
    "# and all members of `tensors` should have the same size in the\n",
    "# first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n",
    "# output will have shape `[batch_size, x, y, z]`.\n",
    "enqueue_many = True\n",
    "\n",
    "cache_dir = os.path.expanduser(\n",
    "    os.path.join('~', '.keras', 'datasets', 'MNIST-data'))\n",
    "data = mnist.read_data_sets(cache_dir, validation_size=0)\n",
    "\n",
    "x_train_batch, y_train_batch = tf.train.shuffle_batch(\n",
    "    tensors=[data.train.images, data.train.labels.astype(np.int32)],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue,\n",
    "    enqueue_many=enqueue_many,\n",
    "    num_threads=8)\n",
    "\n",
    "x_train_batch = tf.cast(x_train_batch, tf.float32)\n",
    "x_train_batch = tf.reshape(x_train_batch, shape=batch_shape)\n",
    "\n",
    "y_train_batch = tf.cast(y_train_batch, tf.int32)\n",
    "y_train_batch = tf.one_hot(y_train_batch, num_classes)\n",
    "\n",
    "x_batch_shape = x_train_batch.get_shape().as_list()\n",
    "y_batch_shape = y_train_batch.get_shape().as_list()\n",
    "\n",
    "model_input = layers.Input(tensor=x_train_batch)\n",
    "model_output = cnn_layers(model_input)\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# Pass the target tensor `y_train_batch` to `compile`\n",
    "# via the `target_tensors` keyword argument:\n",
    "train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    target_tensors=[y_train_batch])\n",
    "train_model.summary()\n",
    "\n",
    "x_test_batch, y_test_batch = tf.train.batch(\n",
    "    tensors=[data.test.images, data.test.labels.astype(np.int32)],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    enqueue_many=enqueue_many,\n",
    "    num_threads=8)\n",
    "\n",
    "# Create a separate test model\n",
    "# to perform validation during training\n",
    "x_test_batch = tf.cast(x_test_batch, tf.float32)\n",
    "x_test_batch = tf.reshape(x_test_batch, shape=batch_shape)\n",
    "\n",
    "y_test_batch = tf.cast(y_test_batch, tf.int32)\n",
    "y_test_batch = tf.one_hot(y_test_batch, num_classes)\n",
    "\n",
    "x_test_batch_shape = x_test_batch.get_shape().as_list()\n",
    "y_test_batch_shape = y_test_batch.get_shape().as_list()\n",
    "\n",
    "test_model_input = layers.Input(tensor=x_test_batch)\n",
    "test_model_output = cnn_layers(test_model_input)\n",
    "test_model = keras.models.Model(inputs=test_model_input, outputs=test_model_output)\n",
    "\n",
    "# Pass the target tensor `y_test_batch` to `compile`\n",
    "# via the `target_tensors` keyword argument:\n",
    "test_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'],\n",
    "                   target_tensors=[y_test_batch])\n",
    "\n",
    "# Fit the model using data from the TFRecord data tensors.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "train_model.fit(\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=int(np.ceil(data.train.num_examples / float(batch_size))),\n",
    "    callbacks=[EvaluateInputTensor(test_model, steps=100)])\n",
    "\n",
    "# Save the model weights.\n",
    "train_model.save_weights('saved_wt.h5')\n",
    "\n",
    "# Clean up the TF session.\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "K.clear_session()\n",
    "\n",
    "# Second Session to test loading trained model without tensors\n",
    "x_test = np.reshape(data.test.images, (data.test.images.shape[0], 28, 28, 1))\n",
    "y_test = data.test.labels\n",
    "x_test_inp = layers.Input(shape=(x_test.shape[1:]))\n",
    "test_out = cnn_layers(x_test_inp)\n",
    "test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n",
    "\n",
    "test_model.load_weights('saved_wt.h5')\n",
    "test_model.compile(optimizer='rmsprop',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "test_model.summary()\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test,\n",
    "                                keras.utils.to_categorical(y_test),\n",
    "                                batch_size=batch_size)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
