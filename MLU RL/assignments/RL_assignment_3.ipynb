{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "# Assignment 3\n",
    "## Introduction\n",
    "Welcome to the third RL assignment.  Last week, we've dived in to Model-Free Prediction & Control with Temporal Difference (TD) and Q-Learning.\n",
    "You've implemented an example of Off-Policy TD Control (SARSA) and another of On-policy TD Control (Q-Learning). <br/> \n",
    "For that, we have represented the value function by a lookup table. <br/>\n",
    "But this approach has a problem: When the underlying Markov decision process is large there are too many states and actions to store in memory. It is extremely difficult to visit all the possible states, meaning that we cannot estimate the values for those states. <br/>\n",
    "\n",
    "For this assignment, we are going to work with generalization: how to produce a good approximation of a large state space experiencing only a small subset, to illustrated how we can apply supervised learning instead of a lokkup table.\n",
    "\n",
    "You will implement a linear version of the TD(0) algorithm and use it to find the utilities, by building a linear function approximation and also applying a Feature Construction for Linear Methods for multiple gridworlds.\n",
    "You also will deal with a non-linear separable problem and learn how to overcome it\n",
    "\n",
    "## Resources\n",
    "I've always found experience the best teacher, so I advocate just diving in and trying to implement things.  However, it is always good to have other sources to reference, so every week I'll place links to things that I think are helpful to learn the material.\n",
    "* [Chapter 9 of this book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) about On-policy Prediction and Approximation is nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Approximation\n",
    "Our brain is a powerful approximator. <br/>\n",
    "We can consider the world as a huge and chaotic state-space, where the correct evaluation of a specific stimulus makes the difference between life and death. <br/>\n",
    "The brain stores information about the environment and allows an effective interaction with it. <br/>\n",
    "Let's suppose that our brain is a massive **lookup table**, which can store in a single neuron (or cell) a single state. This is known as **local representation**. <br/>\n",
    "It describes a hypothetical neuron that responds only to a specific and meaningful stimulus. There is an alternative hypothesis where we can suppose that information is stored in a distributed way, and that each single concept is represented through a pattern of activity. <br/>\n",
    "To understand the difference between the two representations think about a computer keyboard.<br/>\n",
    "In the local representation each single key can codify only a single character. In the distributed representation we can use a combination of keys (e.g. Shift and Ctrl) to associate multiple characters to the same key.\n",
    "\n",
    "From a machine learning perspective, we know that the distributed representation works. The success of deep learning is based on neural networks, which are powerful function approximators. <br/>\n",
    "Moreover, different methods, such as dropout, are tightly related to the distributed representation theory. Now it’s time to go back to reinforcement learning, and see how a distributed representation can solve the problems due to local representation.\n",
    "\n",
    "### Function Approximation Intuition\n",
    "In this assignment, you will use the **robot cleaning example**.<br/>\n",
    "The robot moves in a two-dimensional gridworld: \n",
    " + It has only 4 possible actions available (**forward, backward, left, right**) \n",
    " + Its goal is to reach a charger (green cell) and avoid to fall on stairs (red cell). \n",
    "\n",
    "Let's consider: \n",
    "+ V(s) our usual value function \n",
    "+ Q(s,a) the state-action function. \n",
    "\n",
    "The grid-world is a discrete rectangular state space, having c columns and r rows. \n",
    "\n",
    "Using a tabular approach, we can represent V(s) using a table containing r x c = N elements, where:\n",
    "+ N represents the total number of states. \n",
    "\n",
    "To represent Q(s,a) we need a table of size N x M, where:\n",
    "+ M is the total number of actions. \n",
    "\n",
    "Our value function is a matrix having the same size of the world, whereas for the state-action function we use a matrix having N columns (states) and M rows (actions). <br/>\n",
    "+ In the first case, to get the value we have to access the location of the matrix corresponding to the particular state where we are. \n",
    "+ In the second case, we use the state as index to access the column in the state-action matrix and from that column we return the values of all the available actions.\n",
    "<img src=\"../images/val_aprox_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Note**: The value function is an abstract formulation of utility. That is why in some of the figures you see the term \"utility\" being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to visualize a function approximator as a black box.<br/>\n",
    "The method described below can be used on different approximators and for this reason we can easily apply it to the box content. <br/>\n",
    "The black box takes as input the current state and returns the value of the state or the state-action values. <br/>\n",
    "The main advantage is that we can approximate (with an arbitrary small error) the values using less parameters in respect to the tabular approach. <br/>\n",
    "We can say that the number of elements stored in the vector w is smaller than N, the number of values in the tabular counterpart.\n",
    "\n",
    "<img src=\"../images/val_aprox_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a black box that is approximating a value function, the content of the box is the **Value Function Approximator $\\hat{v}$(s,w)**. \n",
    "\n",
    "Using a more formal view we can say that the vector w is adjusted at every iteration, moving the values of a quantity Δ, in order to reach an objective which is minimizing a function of cost. <br/>\n",
    "The cost is given by an error measure that we can obtain comparing the output of the function with a target. <br/>\n",
    "For instance, suppose the actual value of state (4,1) in our gridworld is 0.388. Let’s say that at time t the output of the box is 0.352. <br/>\n",
    "After the update step, the output will be 0.371, we moved closer to the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function approximation is an instance of supervised learning. \n",
    "\n",
    "In principle all the supervised learning techniques could be used in function approximation. <br/>\n",
    "The vector w may be the set of connection weights of a neural network or the split points and leaf values of a decision tree. \n",
    "For this assignment, however, we will consider a linear combination of features.\n",
    "\n",
    "### Method\n",
    "To improve the performance of our function approximator we need:\n",
    "#### a **error measure** <br/>\n",
    "\n",
    "In reinforcement learning it is often used a reinterpretation of the , like **Mean Square Error** called **Mean Squared Value Error (MSVE)**.\n",
    "The MSVE introduce a distribution μ(s)≥0 that specifies how much we care about each state s. <br/>\n",
    "As function approximator is based on a set of weights w that contains less elements than the total number of states, adjusting a subset of weights means improving the value prediction of some states but loosing precision in others. <br/>\n",
    "The function μ(s) gives us an explicit solution and using it we can rewrite the previous equation as follows:\n",
    "<img src=\"../images/val_aprox_5.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    " \n",
    "#### an **update rule** \n",
    "\n",
    "The update rule for differentiable approximator is **gradient descent**. The gradient is a generalization of the concept of derivative applied to scalar-valued functions of multiple variables. <br/>\n",
    "You can imagine the gradient as the vector that points in the direction of the greatest rate of increase.\n",
    "At each step we adjust the parameter vector w moving a step closer to the valley.\n",
    "<img src=\"../images/val_aprox_6.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "These two components work tightly in the learning cycle of every supervised learning technique. \n",
    "Their use in reinforcement learning is not much different from how they are used in a classification task.<br/>\n",
    "\n",
    "#### the target function\n",
    "We do not have the optimal value function yet. Think about that, having this function would mean we do not need an approximator at all. Moving in our gridworld we could simply call v<sub>$\\pi$</sub> at each time step t and get the actual value of that state. \n",
    "So, we cannot perform the exact update because v<sub>$\\pi$</sub>(S<sub>t</sub>) is unknown, but we can approximate it by substituting U<sub>t</sub> in place of v<sub>$\\pi$</sub>(S<sub>t</sub>). <br/>\n",
    "This yields the following general Stochastic Gradient Descend method for state-value prediction:\n",
    "<img src=\"../images/val_aprox_7.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Linear approximation\n",
    "One of the most important special cases of function approximation is that in which the approximate function, $\\hat{v}$(s,w), is a linear function of the weight vector, w. <br/>\n",
    "Corresponding to every state s, there is a real-valued vector x(s)= (x<sub>1</sub>(s), x<sub>2</sub>(s),...,x<sub>d</sub>(s)), with the same number of components as w. Linear methods approximate the state-value function.\n",
    "<img src=\"../images/val_aprox_8_L.png\" alt=\"Drawing\" style=\"width: 250px;\"/> \n",
    "\n",
    "The vector x(s) is called a feature vector representing state s. We think of a feature as the entirety of one\n",
    "of these functions, and we call its value for a state s a feature of s. <br/>\n",
    "For linear methods, features are **basis functions** because they form a linear basis for the set of approximate\n",
    "functions. Constructing d-dimensional feature vectors to represent states is the same as\n",
    "selecting a set of d basis functions. <br/>\n",
    "\n",
    "It is natural to use SGD updates with linear function approximation. The gradient of the approximate value function with respect to w in this case is\n",
    "<img src=\"../images/val_aprox_9_L.png\" alt=\"Drawing\" style=\"width: 120px;\"/> \n",
    "\n",
    "Thus, in the linear case the general SGD update reduces to a particularly simple\n",
    "form:\n",
    "<img src=\"../images/val_aprox_10_L.png\" alt=\"Drawing\" style=\"width: 300px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boolean Gridworlds \n",
    "Let’s suppose we have a square gridworld where charging stations (green cells) and stairs (red cells) are disposed in multiple locations. The position of the positive and negative cells can vary giving rise to four worlds which I called: OR-world, AND-world, NAND-world, XOR-world. The rule of the worlds are similar to the one defined in the previous posts. The robot has four action available: forward, backward, left, right. When an action is performed, with a probability of 0.2 it can lead to a wrong movement. The reward is positive (+1.0) for green cells, negative (-1.0) for red cells, and null in all the other cases. The index convention for the states is the usual (column, row) where (0,0) represents the cell in the bottom-left corner and (4,4) the cell in the top-right corner.\n",
    "<img src=\"../images/val_aprox_11.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "Recalling your Boolean algebra, you have already noticed that there is a pattern in the worlds which reflects basic Boolean operations. From the geometrical point of view, when we apply a linear approximator to the Boolean worlds, we are trying to find a plane in a three-dimensional space which can discriminate between states with high approximated value (green cells) and states with low approximated value (red cells).\n",
    "\n",
    "<img src=\"../images/val_aprox_12.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "In the three-dimensional space the x-axis is represented by the columns of the world, whereas the y-axis is represented by the rows. The approximated value is given by the z-axis. During the gradient descent we are changing the weights, adjusting the inclination of the plane and the utilities associated to each state.\n",
    "\n",
    "\n",
    "Your assignment implementation is based on a random agent which freely move in the world. Here we are only interested in estimating the state utilities, we do not want to find a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Boolean Gridworld\n",
    "The code below contains functions to initialize the four Boolean gridworlds used in this assignment.\n",
    "#### Note\n",
    "For Questions 1, 2 and 3, you should use them as they are.<br/>\n",
    "For Questions 4 and 5 you will be instructed to change them a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "#MIT License\n",
    "#Copyright (c) 2017 Massimiliano Patacchiola\n",
    "#\n",
    "#Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#of this software and associated documentation files (the \"Software\"), to deal\n",
    "#in the Software without restriction, including without limitation the rights\n",
    "#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#copies of the Software, and to permit persons to whom the Software is\n",
    "#furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#The above copyright notice and this permission notice shall be included in all\n",
    "#copies or substantial portions of the Software.\n",
    "#\n",
    "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "#SOFTWARE.\n",
    "\n",
    "#In this script the TD(0) linear approximator is used to estimate the utilities\n",
    "#of the boolean worlds.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib_rl.gridworld import GridWorld\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Rectangle\n",
    "import mpl_toolkits.mplot3d.art3d as art3d\n",
    "\n",
    "def init_and():\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[-1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [-1.0, 0.0, 0.0, 0.0, -1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    \n",
    "    return env, np.random.uniform(-1, 1, 2)\n",
    "\n",
    "def init_nand():\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[1.0, 0.0, 0.0, 0.0, -1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    \n",
    "    return env, np.random.uniform(-1, 1, 2)\n",
    "\n",
    "def init_or():\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [-1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    \n",
    "    return env, np.random.uniform(-1, 1, 2)\n",
    "\n",
    "def init_xor():\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[1.0, 0.0, 0.0, 0.0, -1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [-1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    \n",
    "    return env, np.random.uniform(-1, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Implement the **Linear Stochastic Gradient Descent (SGD)** function described above in order to return the updated weights associated to each feature, for each learning step:\n",
    "<img src=\"../images/val_aprox_10_L.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "The input parameters and the return value are described below. <br/>\n",
    "**Notes:** \n",
    "+ Use numpy dot product implementation for that.\n",
    "+ In case of a terminal state, the target is obtained using only the reward. This is obvious, because after a terminal state there is not another state to use for approximating the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDupdate(w, x, x_t1, reward, alpha, gamma, done):\n",
    "    \"\"\"\n",
    "    Function that implements the stochastic gradient descent update over the weights. \n",
    "    \n",
    "    Args:\n",
    "        @param w: the weights vector before the update\n",
    "        @param x: the feauture vector obsrved at t\n",
    "        @param x_t1: the feauture vector observed at t+1\n",
    "        @param reward: the reward observed after the action\n",
    "        @param alpha: the ste size (learning rate)\n",
    "        @param gamma: the discount factor\n",
    "        @param done: boolean True if the state is terminal\n",
    "        \n",
    "    Returns:\n",
    "        @return w_t1: the weights vector at t+1\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    \n",
    "    return w_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Now that you have the SGD update function, create a **function approximation** function and use your gradient descent update function to iteratively update the weights through the steps and epochs for one selected Boolean world. <br/>\n",
    "Your function should:\n",
    "+ Start with learning rate α = 0.001 and linearly decrease it to 0.000001 for 3 x 10<sup>4</sup> epochs. Use a discount factor gamma = 0.9. <br/>\n",
    "\n",
    "+ Initialize the weights randomly in the range [-1, +1].\n",
    "\n",
    "+ Use the **env.reset(exploring_starts=True)** function to reset the environment for each epoch and **env.step(action)** function to run each step from the Gridworld environment:\n",
    "+ **env.reset(exploring_starts=True)** will return a ndarray with that represents the initial position in the gridworld.\n",
    "+ **env.step()** will return a ndarray with that represents the next position in the gridworld. (Ex. [2,3])\n",
    "**Note:** the env environment is instantiated in the initialization functions offered above. \n",
    "\n",
    "The function should return the **sum of square errors** and the **final weight vectors**.<br/>\n",
    "\n",
    "Print the MSE and the weights returned by your function.\n",
    "\n",
    "Use the **print_value_function** below to print the approximated values an MSE returned by your approximation function for each gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value_function(w, tot_rows, tot_cols, decimal=2):\n",
    "    \"\"\"Function that prints the approximated value matrix of a discrete state space\n",
    "       having states defined by tuples: (0,0); (0,1); (0,2) ...\n",
    "    Args:\n",
    "        @param w the weights vector\n",
    "        @param tot_rows total number of rows\n",
    "        @param tot_cols total number of columns\n",
    "        @param decimal is the precision of the printing (default: 2 decimal places)\n",
    "        @param flip boolean which defines if vertical flip is applied (default: True)\n",
    "    Returns:\n",
    "        @return : a Pandas dataframe with the approximated values for the boolean gridworld.\n",
    "    \"\"\"\n",
    "    value_matrix = np.zeros((tot_rows, tot_cols))\n",
    "    for row in range(tot_rows):\n",
    "        for col in range(tot_cols):\n",
    "            x = np.ones(w.shape[0])\n",
    "            x[0] = row\n",
    "            x[1] = col\n",
    "            value_matrix[row,col] = np.dot(x,w)\n",
    "    np.set_printoptions(precision=decimal) #set print precision of numpy\n",
    "     \n",
    "    return pd.DataFrame(np.flipud(value_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def approximation_function(world = \"AND\", gamma = 0.9, alpha_start = 0.001, alpha_stop = 0.000001, \n",
    "                           tot_epoch = 30000):\n",
    "    \"\"\"Iterativelly update the weights applying SGD\n",
    "    Args:\n",
    "        @param world: the bollean world to use (ANN, NAND, OR, XOR)\n",
    "        @param gamma:  discaount factor\n",
    "        @param alpha_start: alpha initial value\n",
    "        @param alpha_stop: alpha final value\n",
    "        @param tot_epoch: number of epochs to run\n",
    "    Returns:\n",
    "        @return mse: Mean Squared Error\n",
    "        @return w: linear weights\n",
    "        \n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    \n",
    "    return mse, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### RUN YOUR FUNCTION HERE  ###\n",
    "###############################\n",
    "world = \"AND\"\n",
    "mse_and, w_and = approximation_function(world)\n",
    "\n",
    "print(\"AND world MSE: \" + str(mse_and))\n",
    "print(\"AND World weights: \" + str(w_and))\n",
    "df_value_matrix_and = print_value_function(w_and, tot_rows=5, tot_cols=5)\n",
    "df_value_matrix_and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### RUN YOUR FUNCTION HERE  ###\n",
    "###############################\n",
    "world = \"NAND\"\n",
    "mse_nand, w_nand = approximation_function(world)\n",
    "\n",
    "print(\"NAND world MSE: \" + str(mse_nand))\n",
    "print(\"NAND World weights: \" + str(w_nand))\n",
    "df_value_matrix_nand = print_value_function(w_nand, tot_rows=5, tot_cols=5)\n",
    "df_value_matrix_nand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### RUN YOUR FUNCTION HERE  ###\n",
    "###############################\n",
    "world = \"OR\"\n",
    "#mse_or, w_or = approximation_function(world, use_bias = False)\n",
    "mse_or, w_or = approximation_function(world)\n",
    "\n",
    "print(\"OR world MSE: \" + str(mse_or))\n",
    "print(\"OR World weights: \" + str(w_or))\n",
    "df_value_matrix_or = print_value_function(w_or, tot_rows=5, tot_cols=5)\n",
    "df_value_matrix_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### RUN YOUR FUNCTION HERE  ###\n",
    "###############################\n",
    "world = \"XOR\"\n",
    "mse_xor, w_xor = approximation_function(world)\n",
    "\n",
    "print(\"XOR world MSE: \" + str(mse_xor))\n",
    "print(\"XOR World weights: \" + str(w_xor))\n",
    "df_value_matrix_xor = print_value_function(w_xor, tot_rows=5, tot_cols=5)\n",
    "df_value_matrix_xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Create a heatmap showing the approximated values for each Boolean gridworld. Map negative values to red and positive values to green. \n",
    "\n",
    "For our supervised learning approach point of view, the approximated values matrix should define planes that separates the positive from the negative rewards in a 3D view. In other words, the values should be -1 in proximity of a red cell, and +1 in proximity of a green cell.<br/>\n",
    "+ Do the heatmaps show a pattern that reflect the rewards for each Boolean map? <br/>\n",
    "\n",
    "Use the **3D-plot function** below to have a graphical view of the approximated values related to the 2D grid to confirm visually how the planes are performing in successfully separating the rewards.\n",
    "+ Are the value planes successfully separating the positive rewards from the negative ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmaps for approximated values\n",
    "#################\n",
    "### CODE HERE ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to have a 3-D view of the results, passing a list of weights for each gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(w_list, bool_op_list, world_size, filename=\"figure.png\"):\n",
    "        #Define the main figure property\n",
    "        fig, ax_array = plt.subplots(nrows=1, ncols=len(w_list), subplot_kw={'projection': '3d', 'autoscale_on':False, 'aspect':'equal'})\n",
    "        #Iteration on all the subplots\n",
    "        counter = 0\n",
    "        for ax in ax_array:\n",
    "            w = w_list[counter]\n",
    "            print(\"w: {}\".format(w))\n",
    "            x, y = np.meshgrid(np.arange(0.5, world_size-0.5, 0.5), np.arange(0.5, world_size-0.5, 0.5))\n",
    "            if w.shape[0] == 3:\n",
    "                z = w[0]*x + w[1]*y + w[2]\n",
    "            elif w.shape[0] == 2:\n",
    "                z = w[0]*x + w[1]*y\n",
    "            else:\n",
    "                raise ValueError('[BOOLEAN WORLDS][ERROR] The weight vector has a wrong shape')\n",
    "            ax.clear()\n",
    "            #_add_rectangles(ax, bool_op=bool_op_list[counter])\n",
    "            bool_op = bool_op_list[counter]\n",
    "            if bool_op == 'AND':\n",
    "                color_00 = \"red\"; color_11 = \"green\"; color_10 = \"red\"; color_01 = \"red\"\n",
    "            elif bool_op == 'NAND':\n",
    "                color_00 = \"green\"; color_11 = \"red\"; color_10 = \"green\"; color_01 = \"green\"\n",
    "            elif bool_op == 'OR':\n",
    "                color_00 = \"red\"; color_11 = \"green\"; color_10 = \"green\"; color_01 = \"green\"\n",
    "            elif bool_op == 'XOR':\n",
    "                color_00 = \"red\"; color_11 = \"red\"; color_10 = \"green\"; color_01 = \"green\"\n",
    "            else:\n",
    "                color_00 = \"red\"; color_11 = \"red\"; color_10 = \"red\"; color_01 = \"red\"\n",
    "            #Draw the rectangles\n",
    "            p = Rectangle((0, 0), 1, 1, color=color_00, alpha=0.5)\n",
    "            ax.add_patch(p)\n",
    "            art3d.pathpatch_2d_to_3d(p, z=-1.0, zdir=\"z\")\n",
    "            p = Rectangle((world_size-1, world_size-1), 1, 1, color=color_11, alpha=0.5)\n",
    "            ax.add_patch(p)\n",
    "            art3d.pathpatch_2d_to_3d(p, z=-1.0, zdir=\"z\")\n",
    "            p = Rectangle((0, world_size-1), 1, 1, color=color_01, alpha=0.5)\n",
    "            ax.add_patch(p)\n",
    "            art3d.pathpatch_2d_to_3d(p, z=-1.0, zdir=\"z\")\n",
    "            p = Rectangle((world_size-1, 0), 1, 1, color=color_10, alpha=0.5)\n",
    "            ax.add_patch(p)\n",
    "            art3d.pathpatch_2d_to_3d(p, z=-1.0, zdir=\"z\")\n",
    "            #Set the subplot properties\n",
    "            #ax.tick_params(labelsize=10)\n",
    "            ax.set_xticks(np.arange(0, world_size+1, 1))\n",
    "            ax.set_xticklabels('', fontsize=10)\n",
    "            ax.set_yticklabels('', fontsize=10)\n",
    "            ax.set_yticks(np.arange(0, world_size+1, 1))\n",
    "            ax.set_zlim(-1.0,1.0)\n",
    "            ax.set_zticklabels(['-1.0','','0','','1.0'], fontsize=8)\n",
    "            ax.view_init(elev=30, azim=-115)\n",
    "            ax.plot_surface(x,y,z, color='lightgrey', alpha=0.5)\n",
    "            #Draw a White background\n",
    "            x, y = np.meshgrid(np.arange(0, world_size+1, 1), np.arange(0, world_size+1, 1))\n",
    "            z = x*(-1.0)\n",
    "            ax.plot_surface(x,y,z, color='white', alpha=0.01)\n",
    "            counter += 1\n",
    "\n",
    "        fig.savefig(filename, dpi=300) #, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating plot, please wait...\")\n",
    "plot_3d([w_and, w_nand, w_or, w_xor], [\"AND\", \"NAND\", \"OR\", \"XOR\"], world_size=5, filename=\"boolean_planes.png\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Broken?\n",
    "You should have noticed that something is broken about the results above. <br/>\n",
    "Apparently, the supervised linear approximation is not doing a good job creating the planes that separates the positive from the negative rewards for the four gridworlds.\n",
    "\n",
    "The values should be -1 in proximity of a red cell, and +1 in proximity of a green cell. However, examining the plot we can notice that something strange is happening. The planes are flat and the resulting approximated values is always close to zero in all the worlds but the OR-world. Overall, it seems that the approximator is not working at all and that its output is always null. \n",
    "### What is going on? \n",
    "Our current definition of approximator does not take into account an important factor, the translation of the plane. Having only two weights we can rotate the surface on the xy-plane but we cannot translate it up and down. Only in the OR-world it is possible to adjust the inclination and satisfy all the constraints.\n",
    "\n",
    "### How do we Fix it?\n",
    "We have to introduce the **bias unit**. <br/>\n",
    "The bias unit, that you may be familiar with if you know regression and neural network models, can be represented as an additional input which is always equal to 1. <br/>\n",
    "Using the bias unit the input vector becomes x = { x<sub>1</sub>,x<sub>2</sub>, ..., x<sub>N</sub>, x<sub>b</sub>}, with x<sub>b</sub> = 1. <br/>\n",
    "At the same time we have to add an additional value in the weight vector w = { w<sub>1</sub>,w<sub>2</sub>, ..., w<sub>N</sub>, w<sub>b</sub>}. The additional weight w<sub>b</sub> is updated similarly to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Modify your model to incorporate this bias term and see if it fixes the problem. <br/>\n",
    "\n",
    "Follow the steps below:\n",
    "\n",
    "1. Change the **environment initialization functions** below, that you have used for questions 1, 2 and 3, to receive a Boolean parameter indicating if you are using or not the bias term. Basically, you need to change the second return value to incorporate the additional dimension related to the bias term into the array returned for each observation. \n",
    "2. Change the **approximation function** for question 2 to receive a boolean parameter indicating if you are using or not the bias term, call the initialization function passing this parameter. Name it **approximation_function_with_bias**. Basically, you will only need to change the return values for **reset** and **step** functions that you are using from the Gridworld environment, to incorporate the new dimension for the returned coordinates of each observation.  \n",
    "\n",
    "To confirm if your model has solved the issue:\n",
    "1. Run the approximation function for all Boolean gridworld\n",
    "2. Plot the approximated values heatmaps for each Boolean gridworld to see if the bias term solved the problem.\n",
    "3. Use the 3D-plot function below to have a graphical view of the value function related to the 2D grid. <br/>\n",
    "\n",
    "**Did the bias term solved the problem observed in the previous question?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_and(bias=True):\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[-1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [-1.0, 0.0, 0.0, 0.0, -1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    return env, observation\n",
    "\n",
    "def init_nand(bias=True):\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[1.0, 0.0, 0.0, 0.0, -1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    return env, observation\n",
    "\n",
    "def init_or(bias=True):\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [-1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    return env, observation\n",
    "\n",
    "def init_xor(bias=True):\n",
    "    '''Init the boolean environment\n",
    "\n",
    "    @return the environment gridworld object\n",
    "    '''\n",
    "    env = GridWorld(5, 5)\n",
    "    #Define the state matrix\n",
    "    state_matrix = np.array([[1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                             [1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the index matrix\n",
    "    index_matrix = np.array([[(4,0), (4,1), (4,2), (4,3), (4,4)],\n",
    "                             [(3,0), (3,1), (3,2), (3,3), (3,4)],\n",
    "                             [(2,0), (2,1), (2,2), (2,3), (2,4)],\n",
    "                             [(1,0), (1,1), (1,2), (1,3), (1,4)],\n",
    "                             [(0,0), (0,1), (0,2), (0,3), (0,4)]])\n",
    "    #Define the reward matrix\n",
    "    reward_matrix = np.array([[1.0, 0.0, 0.0, 0.0, -1.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                              [-1.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "    #Define the transition matrix\n",
    "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                                  [0.1, 0.8, 0.1, 0.0],\n",
    "                                  [0.0, 0.1, 0.8, 0.1],\n",
    "                                  [0.1, 0.0, 0.1, 0.8]])\n",
    "    env.setStateMatrix(state_matrix)\n",
    "    env.setIndexMatrix(index_matrix)\n",
    "    env.setRewardMatrix(reward_matrix)\n",
    "    env.setTransitionMatrix(transition_matrix)\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    return env, observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def approximation_function_with_bias(world = \"AND\", gamma = 0.9, alpha_start = 0.001, alpha_stop = 0.000001, \n",
    "                           tot_epoch = 30000, use_bias = True):\n",
    "    \"\"\"Iterativelly update the weights applying SGD\n",
    "    Args:\n",
    "        @param world: the bollean world to use (ANN, NAND, OR, XOR)\n",
    "        @param gamma:  discaount factor\n",
    "        @param alpha_start: alpha initial value\n",
    "        @param alpha_stop: alpha final value\n",
    "        @param tot_epoch: number of epochs to run\n",
    "        @param use_bias: boolean indicanting the use of the bias term\n",
    "    Returns:\n",
    "        @return mse: Mean Squared Error\n",
    "        @return w: linear weights\n",
    "        \n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "\n",
    "    return mse, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### RUN YOUR FUNCTION HERE  ###\n",
    "###############################\n",
    "world = \"AND\"\n",
    "mse_and, w_and = approximation_function_with_bias(world, use_bias = True)\n",
    "world = \"NAND\"\n",
    "mse_nand, w_nand = approximation_function_with_bias(world, use_bias = True)\n",
    "world = \"OR\"\n",
    "mse_or, w_or = approximation_function_with_bias(world, use_bias = True)\n",
    "world = \"XOR\"\n",
    "mse_xor, w_xor = approximation_function_with_bias(world, use_bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmaps for value functions with bias\n",
    "#################\n",
    "### CODE HERE ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating plot, please wait...\")\n",
    "plot_3d([w_and, w_nand, w_or, w_xor], [\"AND\", \"NAND\", \"OR\", \"XOR\"], world_size=5, filename=\"boolean_planes.png\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 Solution\n",
    "### Why it worked?\n",
    "The planes are no more flat, because introducing the bias gave us the possibility to shift them up and down. Now the planes can be adjusted to fit all the constraints. <br/>\n",
    "For this reason, the matrix has been vertically flipped in order to match the values with the cells of the gridworld. Giving a look to the utilities we can see that in most of the worlds they are pretty good.<br/>\n",
    "At this point it should be clear why having a function approximator is a big deal. With the lookup table approach we could represent the approximated values of the Boolean worlds using a table with 5 rows and 5 columns, for a total of 25 variables to keep in memory. Now we only need two weights and a bias, for a total of 3 variables. Everything seems fine, we have an approximator which works pretty well and is easy to tune. <br/>\n",
    "However, our problems are not finished.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Broken?\n",
    "You may have observed that the **bias term** worked well separating the positive from the negative reward for the AND, NAND and OR gridworlds, except for the XOR gridworld.\n",
    "### What is going on?\n",
    "For the XOR-world the plane is still flat. This problem is much serious than the previous one and there is no way to solve it keeping the plane flat. There is no plane that can separate red and green cells in the XOR-world.<br/>\n",
    "**The XOR-world is not linearly separable!**\n",
    "### How do we fix it?\n",
    "The only chance we have to approximate a value function for the XOR-world is to literally bend the plane, and to do it we have to use a higher order approximator.\n",
    "The linear approximator is the simplest form of approximation. The linear case is appealing not only for its simplicity but also because it is guaranteed to converge. However, there is an important limit implicit in the linear model: it cannot represent complex relationships between features.<br/>\n",
    "Such a complex interaction naturally arises in physical systems. Some features may be informative only when other features are absent. For example, the inverted pendulum angular position and velocity are tightly connected. A high angular velocity may be either good or bad depending on the position of the pole. If the angle is high then high angular velocity means an imminent danger of falling, whereas if the angle is low then high angular velocity means the pole is righting itself.\n",
    "\n",
    "Solving the XOR problem is very easy when an additional feature is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Change the last approximation function that you've created (approximation_function_with_bias), adding an additional higher order feature by multiplying the two previous used: **x<sub>1</sub>** and **x<sub>2</sub>**. \n",
    "Basically, you will need to:\n",
    "+ Set the parameter **use_bias** = True \n",
    "+ Change the coordinates of each observation array returned by the **env.step** function, to incorporate one more dimension: the **x<sub>1</sub>** and **x<sub>2</sub>** new feature.\n",
    "**Note:** remember that you have already incorporated the bias term in this array for the previous question. Keep it there.\n",
    "\n",
    "To confirm if your model has solved the issue:\n",
    "1. Run the approximation function for the **XOR** gridworld\n",
    "2. Plot the approximated values heatmaps for the XOR gridworld to see if the bias term solved the problem.\n",
    "3. Use the 3D-plot function below to have a graphical view of the value function related to the 2D grid. <br/>\n",
    "\n",
    "**Did the higher order term solved the problem observed in the previous question?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def approximation_function_high_order(world = \"XOR\", gamma = 0.9, alpha_start = 0.001, alpha_stop = 0.000001, \n",
    "                           tot_epoch = 30000, use_bias = True):\n",
    "    \"\"\"Iterativelly update the weights applying SGD\n",
    "    Args:\n",
    "        @param world: the bollean world to use (ANN, NAND, OR, XOR)\n",
    "        @param gamma:  discaount factor\n",
    "        @param alpha_start: alpha initial value\n",
    "        @param alpha_stop: alpha final value\n",
    "        @param tot_epoch: number of epochs to run\n",
    "        @param use_bias: boolean indicanting the use of the bias term\n",
    "    Returns:\n",
    "        @return mse: Mean Squared Error\n",
    "        @return w: linear weights\n",
    "        \n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    \n",
    "    return mse, w_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### RUN YOUR FUNCTION HERE  ###\n",
    "###############################\n",
    "mse_xor, w_quadratic = approximation_function_high_order(world = \"XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to print the value function returned by your approximation function for the XOR grigworld with the higher oredr term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value_function_xor(w, tot_rows, tot_cols, decimal=2):\n",
    "    '''Print on terminal the approximated value matrix of a discrete state space\n",
    "       having states defined by tuples: (0,0); (0,1); (0,2) ...\n",
    "\n",
    "    @param w the weights vector\n",
    "    @param tot_rows total number of rows\n",
    "    @param tot_cols total number of columns\n",
    "    @param decimal is the precision of the printing (default: 2 decimal places)\n",
    "    @param flip boolean which defines if vertical flip is applied (default: True)\n",
    "    '''\n",
    "    value_matrix = np.zeros((tot_rows, tot_cols))\n",
    "    for row in range(tot_rows):\n",
    "        for col in range(tot_cols):\n",
    "            x = np.ones(w.shape[0])\n",
    "            x[0] = row\n",
    "            x[1] = col\n",
    "            x[2] = row * col\n",
    "            x[3] = 1.0\n",
    "            value_matrix[row,col] = np.dot(x,w)\n",
    "    np.set_printoptions(precision=decimal) #set print precision of numpy\n",
    "    \n",
    "    np.set_printoptions(precision=8) #reset to default\n",
    "    \n",
    "    return pd.DataFrame(np.flipud(value_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmaps for value functions with higher order term\n",
    "#################\n",
    "### CODE HERE ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib_rl.xor_paraboloid_td as xor\n",
    "print(\"Generating plot, please wait...\")\n",
    "xor.subplot(w_quadratic, world_size=5, filename=\"xor_planes.png\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
