{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "# Assignment 5\n",
    "## Introduction\n",
    "Welcome to the fifth RL assignment.  This week, we'll dive into Multi-Armed Bandits and how they're used to solve problems in experimentation.  The main issue is that when you run an experiment you don't want to serve poor performing treatments longer than necessary.  **Multi-Armed Bandits** is a framework solution to this.\n",
    "\n",
    "## Resources\n",
    "I've always found experience the best teacher, so I advocate just diving in and trying to implement things.  However, it is always good to have other sources to reference, so every week I'll place links to things that I think are helpful to learn the material.\n",
    "* [Chapter 2 of this book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) about Multi-armed Bandits is nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Python Classes and Objects\n",
    "As you have already realized, for Reinforcement Learning, agents have central roles.\n",
    "For this assignment, you will be working with different agent types and Python has an elegant way to deal with objects like our agent.\n",
    "Classes and objects in python are powerful tools. \n",
    "\n",
    "**But Don't be scared**: You don't need to know about classes and objects to solve this assignment. Most of the code will be ready for you. \n",
    "\n",
    "Nevertheless, if you are curious and want to know better about the subject, I recommend that you spend some time running this quick tutorial [https://www.programiz.com/python-programming/class] as a way to get familiar with the Class concept and explore the assignment code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "You are responsible for the creation of a recommendation system where the customer will be offered with different movie genres and you need to predict which ones will have more likelihood to be accepted.\n",
    "You've decided that a reinforcement learning model would be a good approach, in a scenario of unique decision making, where the feedback information is useful as a feedback signal (reward) for a stateless bandits-like RL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution: Multi-Armed Bandits\n",
    "In this unique decision make scenario, the feedback information is used to reinforce our algorithm learning in a stateless approach, bandits-like.\n",
    "A simple way to model this is considering a +1 reward when the recommendation is accepted by the customer, 0 when it is ignored. \n",
    "\n",
    "We could penalize the recommender agent if the customer decides that they don't want to receive certain types of recommendations, but this is a much more complex problem to generalize and we will keep it out of the scope of this assignment.\n",
    "\n",
    "You are going to help build different agents, each one applying different approaches of action choices and comparing the way they learn based on experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Environment\n",
    "RL, and especially Multi-Armed Bandits (MABs), depends on an environment. MABs learn by trying different possibilities, seeing the results, then using that new information to inform the next decision. So when testing and trying different models, we need to simulate an environment.<br/>\n",
    "We are going to create a class for our environment, called **Bandits Problem**, that will have a funtion **execute** to simulate a customer responding to our offer (acception or not accepting the offer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a class to encapsulate the definition of a $k$-Armed Bandits problem.\n",
    "\n",
    "Basically, a **class** is a template used to create objects that have states and that can perform actions. \n",
    "\n",
    "So, classes are perfect to represent an agent, with its states and actions.\n",
    "\n",
    "The class itself doens't have existence in our computer environment, until is is pointed to an object (in this case, we say that we **instantiate** the object).\n",
    "The object is an actual entity that you can interact through your python code.\n",
    "The great thing about using classes is that you can create a **parent** Agent class and then start creating **children** classes for more particular agents that **inherite** all properties and functions form the parent class. <br/>\n",
    "You will the beaulty of that ahead, when we will be using combined parent and child classes to implement the bandit algorithm below:\n",
    "<img src=\"../images/RL-assign4-12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bandits Problem Class\n",
    "First we will create a Bandits Problem class, to represent our simulated environment.\n",
    "You can see below that this class offers three functions that belongs to the associated object:\n",
    "\n",
    "+ **\\__init__** : Class functions surrounded by double underscore are called special functions as they have special meaning. Of one particular interest is the __init__() function. This special function gets called whenever a new object of that class is instantiated and, in our case, we are initializing the internal variables k and q. \n",
    "Note: the self prefix only tells us that this is an internal variable for the object.\n",
    "+ **execute**: Each action has an unknown probability of acceptance by the customer. This function simulates this behavior, rewarding the agent with + 1 when it is accepted, 0 otherwise.\n",
    "\n",
    "+ **optimal**: Returns the optimal accumulated reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditsProblem:\n",
    "\n",
    "    def __init__(self, k=10):\n",
    "        # constructor that initializes k (number of arms) and q (acummulated reward)\n",
    "        self.k = k\n",
    "        self.q = np.random.uniform(size=k)\n",
    "\n",
    "    def execute(self, a, times=1):\n",
    "        return np.random.binomial(times, self.q[a])\n",
    "    \n",
    "    def optimal(self):        \n",
    "        return np.argmax(self.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created below (instantiated) the object **problem**.\n",
    "Below we exemplify how you can access the object's **attributes** (variables) and call its **functions** (also called methods).\n",
    "You can look at the class definition to see the input parameters and the return of the function. \n",
    "\n",
    "**Obs**: The **self** input parameter that appears in all functions in the class definition shouldn't appear when you call the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = BanditsProblem() # instantiate the object\n",
    "# get the number of arms object's attribute and print it\n",
    "print(\"k arms: {}\".format(problem.k)) \n",
    "# Call object's function execute and print the returned value\n",
    "print(\"simulated reward: {}\".format(problem.execute(1, times=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below returns the optimal action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"optimal accumulated reward: {}\".format(problem.optimal()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent Class\n",
    "As we have done with the Environment, below we define the **Agent class** to represent the agent that interacts with the environment. <br/>\n",
    "You can see that it receives as a parameter the object **problem** (an instance of our Environment represented by the **Bandits Problem class**), so any object agent will have access to the environment. <br/>\n",
    "\n",
    "Basically, our **Agent class** accumulates rewards over time and calculates the evolution of the average rewards obtained.\n",
    "\n",
    "This class will be our **parent class** and will perform most of the job. <br/>\n",
    "It offers six functions:\n",
    "\n",
    "+ **\\__init__** : This is used to receive the enviroment (the problem object) and initialize  the internal vectors for accumulated average rewards and optimal values over the rounds. \n",
    "+ **choose_action**: not implemented here, as any particular children agent will implement its own **choose_action** function, we don't need to implementeit now (don't bother about it now!).\n",
    "+ **reward**: Returns reward for a particular round. \n",
    "+ **play**: The function that runs the rounds. It receives the number of rounds to play, run and accumulate the *average rewards* and the *average optimal action* for each round into the vectors *average_reward* and *average_optimal*.\n",
    "+ **rewards**: Just used to return the vector *average_reward* accumulated over all rounds for plotting purposes. \n",
    "+ **optimal_choices_avg**: Just used to return the vector *average_optimal* accumulated over all rounds for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.average_reward = [0]\n",
    "        self.average_optimal = [0]\n",
    "\n",
    "    def choose_action(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reward(self, a, r):\n",
    "        pass\n",
    "    # after N plays, return the average rewards and the average optimal action for each round\n",
    "    def play(self, plays=1000):\n",
    "        \n",
    "        for t in range(1, plays+1):            \n",
    "            # choose action (which arm to pull) - overrided by the type of action that you implement!\n",
    "            a = self.choose_action()            \n",
    "            # returns the reward \n",
    "            r = self.problem.execute(a)\n",
    "            # reward below not execute (pass)\n",
    "            self.reward(a, r)            \n",
    "            # save last average reward (last play)\n",
    "            last_avg = self.average_reward[t-1]                        \n",
    "            # update average reward\n",
    "            current_avg = last_avg + (1. / t) * (r - last_avg)\n",
    "            self.average_reward.append(current_avg)\n",
    "            #updte average optimal action\n",
    "            opt = 1 if a == self.problem.optimal() else 0\n",
    "            update = self.average_optimal[t-1] + (1. / t) * (opt - self.average_optimal[t-1])\n",
    "           \n",
    "            self.average_optimal.append(update)\n",
    "        \n",
    "    def rewards(self):\n",
    "        return self.average_reward[1:]\n",
    "\n",
    "    def optimal_choices_avg(self):\n",
    "        return self.average_optimal[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Random Agent Class\n",
    "This is the first **child class** that we are going to create based on our **parent class** Agent.\n",
    "\n",
    "The agent can choose actions at will. Agents can even act randomly. \n",
    "The class below receives as parameter the object **Agent**. <br/>\n",
    "This means that an object instantiated from the class **RandomAgent** has access to all the attributes and functions of the object Agent, too.<br/>\n",
    "For instance, the attribute k (number of arms) from the Agent class, is now an attribute also for the class **RandomAgent** and so, you can access its content by using the construction:  **self.problem.k**.\n",
    "\n",
    "This is called **inheritance** in Object Oriented Programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "The function **choose_action** uses the k value from the object Agent. Use this value to return a random action value between 0 and k-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    # return one random \"arm\"\n",
    "    def choose_action(self):\n",
    "        #################\n",
    "        ### CODE HERE ###\n",
    "        #################\n",
    "        \n",
    "        #################\n",
    "        ### CODE ENDS ###\n",
    "        #################\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below several times to see if your function is returning a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate the object\n",
    "test_random_agent = RandomAgent(problem)\n",
    "# call the function\n",
    "test_random_agent.choose_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RandomAgentExperiment Class\n",
    "To roughly assess the relative effectiveness of this Random Bandit method, we compared it numerically on a suite of test problems. This is a set of 2000 randomly generated k-armed bandit problems with k = 10. We call this suite of test tasks the **10-armed testbed**.\n",
    "To run these experiments, we are defining the **RandomAgentExperiment class** below.\n",
    "\n",
    "It offers two functions:\n",
    "\n",
    "+ _init_ : Used to pass to the object the parameters received (number of plays for each Bandit method tested, number of experiments) and initialize the average reward vector with zeros.\n",
    "+ run: run the experiment 2000 times by default, for the agent selected (in this case RandomAgent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgentExperiment:\n",
    "\n",
    "    def __init__(self, plays=1000, num_experiments=2000):\n",
    "        self.plays = plays\n",
    "        self.num_experiments = num_experiments\n",
    "        self.avg = np.zeros(plays)\n",
    "\n",
    "    def run(self):\n",
    "        for i in range(1, self.num_experiments + 1):            \n",
    "            problem = BanditsProblem()\n",
    "            agent = RandomAgent(problem)\n",
    "            agent.play(self.plays)\n",
    "            td = np.subtract(agent.rewards(), self.avg) # Temporal difference\n",
    "            self.avg = np.add(self.avg, td / i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are running the **10-armed testbed** and ploting the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = 1000\n",
    "x = range(0, plays)\n",
    "experiment = RandomAgentExperiment()\n",
    "experiment.run()\n",
    "experiment.avg.shape\n",
    "handle, = plt.plot(x, experiment.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it stabilizes around .50 as we are simulating a random response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Value Based Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better informed agent can estimate the average reward for each action and choose between selecting known good actions (exploit) or to explore with a small probability $\\epsilon$.\n",
    "\n",
    "Now we create the **ValueBasedAgent class** to represent this better informed agent. What makes this class smarter is the **reward function**, that were not implemented in our original **Agent class**<br/>\n",
    "\n",
    "**Note**: Like the **RandomAgent**, it also receives the **Agent class** and, as the Agent class receives the **BanditsProblem class**, this new class has full access to all properties and functions to act on the environment. <br/>\n",
    "\n",
    "It offers three functions:\n",
    "\n",
    "+ **\\__init__** : Used to receive the enviroment (the problem object) and initialize with zeros the internal vector for the number of times each action was selected (n). \n",
    "+ **initialize**:  Use to initialize with zeros the internal vector of acummulated reward (q).\n",
    "+ **reward**: Update n and q. \n",
    "\n",
    "The **reward** function implements the following part of out bandit algorithm:\n",
    "<img src=\"../images/RL-assign4-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the **reward** function for the **ValueBasedAgent class** below.\n",
    "Consider that:\n",
    "+ A is the parameter **a**\n",
    "+ R is the parameter **r**\n",
    "+ Your vector $N$ is an internal attribute of your class and can be accessed like this: self.n\n",
    "+ Your vector $Q$ is an internal attribute of your class and can be accessed like this: self.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueBasedAgent(Agent):\n",
    "\n",
    "    def __init__(self, problem):\n",
    "        Agent.__init__(self, problem)\n",
    "        self.n = np.zeros(self.problem.k) # Number of times each action was selected\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self, init=0.0): # Average reward for each action\n",
    "        self.q = np.zeros(self.problem.k) # Average reward for each action\n",
    "        \n",
    "    def reward(self, a, r):\n",
    "        #################\n",
    "        ### CODE HERE ###\n",
    "        #################\n",
    "        \n",
    "        #################\n",
    "        ### CODE ENDS ###\n",
    "        #################\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EpsilonGreedyAgent Class\n",
    "\n",
    "We are going to use our **ValueBasedAgent class** above as our parent class to implement our **EpsilonGreedyAgent Class**.\n",
    "For that, we pass the **ValueBasedAgent class** as a parameter, and then our new class will have acces to all the previous parent classes capabilities (BanditsProblem, Agent and ValueBasedAgent).\n",
    "\n",
    "Using the better informed **ValueBasedAgent class**, we will implement the $\\epsilon- greedy$ algorithm in our **choose_action**.<br/>\n",
    "\n",
    "It offers three functions:\n",
    "\n",
    "+ **\\__init__** : Used to receive the enviroment (the problem object) and the epsilon paramater. \n",
    "+ **\\__str__**:  And interesting special function that is called when you print the object (you can try this!).\n",
    "+ **choose_action**: Implements the $\\epsilon-greedy$ algorithm. \n",
    "\n",
    "The **choose_action** function implements the following part of out bandit algorithm:\n",
    "<img src=\"../images/RL-assign4-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(ValueBasedAgent):\n",
    "\n",
    "    def __init__(self, problem, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        ValueBasedAgent.__init__(self, problem)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'e-greedy, e=%.g' % self.epsilon\n",
    "\n",
    "    def choose_action(self):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            return np.argmax(self.q)\n",
    "        else:\n",
    "            return np.random.randint(self.problem.k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EpsilonGreedyAgentExperiment Class\n",
    "The same way we have done for the **RandomAgentExperiment**, let's roughly assess the relative effectiveness of the greedy and ε-greedy methods, we compared them numerically on a suite of test problems. This is a set of 2000 randomly generated k-armed bandit problems with k = 10. We call this suite of test tasks the **10-armed testbed**.\n",
    "To run these experiments, we are defining the **EpsilonGreedyAgentExperiment  class** below.\n",
    "\n",
    "It offers two functions:\n",
    "\n",
    "+ _init_ : Used to pass to the object the parameters received (epsilon, number of plays for each Bandit method tested, number of experiments) and initialize the average reward vector with zeros.\n",
    "+ run: run the experiment 2000 times by default, for the agent selected (in this case EpsilonGreedyAgent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgentExperiment:\n",
    "\n",
    "    def __init__(self, epsilon=0.1, plays=1000, num_experiments=2000):\n",
    "        self.epsilon = epsilon\n",
    "        self.plays = plays\n",
    "        self.num_experiments = num_experiments\n",
    "        self.avg = np.zeros(plays)\n",
    "\n",
    "    def run(self):\n",
    "        for i in range(1, self.num_experiments + 1):\n",
    "            \n",
    "            problem = BanditsProblem()\n",
    "            agent = EpsilonGreedyAgent(problem, self.epsilon)\n",
    "            agent.play(self.plays)\n",
    "         \n",
    "            td = np.subtract(agent.rewards(), self.avg) # Temporal difference\n",
    "            self.avg = np.add(self.avg, td / i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Different values of $\\epsilon$ make a difference in the average performance obtained over time. \n",
    "Run the 10-armed testbed experiment defined by **EpsilonGreedyAgentExperiment class**. Try with epsilon = .1, 0.01 and 0 (when using 0, we call it a greedy method).<br/> \n",
    "\n",
    "Steps for each epsilon value tested:\n",
    "+ Instantiate and object **experiment** with the **EpsilonGreedyAgentExperiment class** for 2000 experiments and 1000 plays.\n",
    "+ Call the function **run()** for this object.\n",
    "+ Plot the average performance of $\\epsilon-greedy$ action-value methods returned (attribute **avg** of the object **experiment**) against the number of plays.\n",
    "**Note**: These ploted values are averages over 2000 runs with different bandit problems. \n",
    "**Optional**: Combine the plots in one figure to better compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def E_greedy_Experiment():\n",
    "    #################\n",
    "    ### CODE HERE ###\n",
    "    #################\n",
    "    \n",
    "    #################\n",
    "    ### CODE ENDS ###\n",
    "    #################\n",
    "\n",
    "%time E_greedy_Experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BiasedEpsilonGreedyAgent Class\n",
    "Optimistic agents are encouraged to explore more, so initialization of the action function provides us with some opportunities for tuning the agent's initial behavior.\n",
    "\n",
    "We can compare the performance of both approaches in the experiment below. The realistic agent assumes the initial values are all zeros, while an optimistic agent has all its estimates initialized to +5.\n",
    "For that, we are defining the **BiasedEpsilonGreedyAgent class** below.\n",
    "\n",
    "We are going to use our **EpsilonGreedyAgent class** above as a parent class to implement our **BiasedEpsilonGreedyAgent  Class**.\n",
    "For that, we pass the **EpsilonGreedyAgent class** as a parameter, and then our new class will have acces to all the previous parent classes capabilities (BanditsProblem, Agent and ValueBasedAgent).\n",
    "\n",
    "It offers two functions:\n",
    "\n",
    "+ **\\_init_** : Used to pass to the object the parameter received (bias) and initialize the **EpsilonGreedyAgent** with the epsilon value received.\n",
    "+ **initialize**: Use to initialize with the bias value the internal vector of acummulated reward (q).\n",
    "+ **run**: run the experiment 2000 times by default, for the agent selected (in this case EpsilonGreedyAgent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedEpsilonGreedyAgent(EpsilonGreedyAgent):\n",
    "\n",
    "    def __init__(self, problem, bias=0, epsilon=0.1):\n",
    "        self.bias = bias\n",
    "        EpsilonGreedyAgent.__init__(self, problem, epsilon)\n",
    "\n",
    "    def initialize(self):\n",
    "        self.q = np.ones(self.problem.k) * self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "+ Instantiate two agents objects from the same **BiasedEpsilonGreedyAgent class**, one realistic (action function initialized with zeroes) and another optimistics (action function initialized with 5.0).<br/>\n",
    "**Tip**: look how we have passed the parameters to the **EpsilonGreedyAgent** when using the function **evaluate**.\n",
    "+ print the attribute **q** of each object to confirm that it have worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### CODE HERE ###\n",
    "#################\n",
    "\n",
    "#################\n",
    "### CODE ENDS ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a **AgentBiasExperiment class** that is pretty similar to the previous **AgentExperiment class**.\n",
    "Actually, the only difference is that, instead of receiving only the epsilon parameter, this class receives the **param** tuple with (bias, epsilon). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBiasExperiment:\n",
    "\n",
    "    def __init__(self, params, plays=1000, num_experiments=2000):\n",
    "        self.params = params\n",
    "        self.plays = plays\n",
    "        self.num_experiments = num_experiments\n",
    "        self.avg = np.zeros(plays)\n",
    "\n",
    "    def run(self):\n",
    "        for i in range(1, self.num_experiments + 1):\n",
    "            problem = BanditsProblem()\n",
    "            bias, epsilon = self.params\n",
    "            agent = BiasedEpsilonGreedyAgent(problem, bias, epsilon)\n",
    "            agent.play(self.plays)\n",
    "            td = np.subtract(agent.optimal_choices_avg(), self.avg) # Optimal action %\n",
    "            self.avg = np.add(self.avg, td / i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different initialization values for the action function make a difference in the average performance obtained over time. \n",
    "\n",
    "Run the 10-armed testbed experiment defined by **AgentBiasExperiment class** below. We are passing two tuples for (bias, epsilon): (0,0.1) and (5, 0.1).<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiasExperiment():\n",
    "    plays = 1000\n",
    "    x = range(0, plays)\n",
    "    handles = []\n",
    "    for params in [(0, 0.1), (5, 0.1)]:\n",
    "        np.random.seed(0)\n",
    "        experiment = AgentBiasExperiment(params, plays)\n",
    "        experiment.run()\n",
    "        handle, = plt.plot(x, experiment.avg, label=params)\n",
    "        handles.append(handle)\n",
    "        \n",
    "        plt.legend(handles=handles, loc=4)\n",
    "    \n",
    "%time BiasExperiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper-Confidence-Bound Action Selection\n",
    "\n",
    "Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better. ε-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. <br/>\n",
    "It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. <br/>\n",
    "One effective way of doing this is to select actions according to\n",
    "\n",
    "$$A_t = \\textrm{arg} \\max_a \\left[Q_t(a) + c \\sqrt{\\frac{\\log t}{N_t(a)}} \\right]$$\n",
    "\n",
    "+ $N_t(a)$ denotes the number of times that action a has been selected prior to time\n",
    "t\n",
    "+ c > 0 controls the degree of exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The UcbAgent Class\n",
    "This class will implement an UCB agent, taking into account the uncertainty of estimates about actions being optimal.\n",
    "\n",
    "We are going to use our **ValueBasedAgent class** as a parent class to implement our **BiasedEpsilonGreedyAgent  Class**, as we have done with the **EpsilonGreedyAgent class**  previously.\n",
    "For that, we pass the **EpsilonGreedyAgent class** as a parameter, and then our new class will have acces to all the previous parent classes capabilities (BanditsProblem, Agent and ValueBasedAgent).\n",
    "\n",
    "It offers four functions:\n",
    "\n",
    "+ **\\_init_** : Used to pass to the object the parameter received (c) and initialize the time step (t).\n",
    "\n",
    "+ **run**: run the experiment 2000 times by default, for the agent selected (in this case EpsilonGreedyAgent).\n",
    "+ **\\__str__**:  And interesting special function that is called when you print the object (you can try this!).\n",
    "+ **choose_action**: Implements the UCB action selection equation.\n",
    "+ **reward**: Implements the reward function from the parent **ValueBasedAgent class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UcbAgent(ValueBasedAgent):\n",
    "\n",
    "    def __init__(self, problem, c=1):\n",
    "        ValueBasedAgent.__init__(self, problem)\n",
    "        self.t = 0\n",
    "        self.c = c  # the confidence value\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'UCB, c=%.g' % self.c\n",
    "\n",
    "    def choose_action(self):\n",
    "        \n",
    "        uneval = np.where(self.n == 0)[0]\n",
    "        if uneval.size == 0:\n",
    "            #################\n",
    "            ### CODE HERE ###\n",
    "            #################\n",
    "            \n",
    "            #################\n",
    "            ### CODE ENDS ###\n",
    "            #################\n",
    "            \n",
    "            return best_action\n",
    "        else:\n",
    "            return uneval[0]\n",
    "\n",
    "    def reward(self, a, r):\n",
    "        ValueBasedAgent.reward(self, a, r)\n",
    "        self.t = self.t + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Implement the UCB action equation above into the **choose_action** function of the **UcbAgent class** below.\n",
    "Consider that:\n",
    "+ A is the parameter **a**\n",
    "+ R is the parameter **r**\n",
    "+ Your vector $N_t$ is an internal attribute of your class and can be accessed like this: self.n\n",
    "+ Your vector $Q_t$ is an internal attribute of your class and can be accessed like this: self.q\n",
    "+ Your degree of exploration $c$ is an internal attribute of your class and can be accessed like this: self.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As a final exercise, we willplot the accumulated rewards (the **agent.rewards** function returns the accumulated rewards for each 1.000 plays) for the following agents:\n",
    "+ UcbAgent, using a confidence interval = 0.5.\n",
    "+ UcbAgent, using the default confidence interval = 1.\n",
    "+ UcbAgent, using a confidence interval = 2.\n",
    "+ EpsilonGreedyAgent, using epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_turns = 1000\n",
    "x = range(0, num_turns)\n",
    "handles = []\n",
    "np.random.seed(1)\n",
    "for agent in [UcbAgent(problem), UcbAgent(problem, 2), \n",
    "              UcbAgent(problem, 0.5), EpsilonGreedyAgent(problem, 0.1)]:\n",
    "    agent.play(num_turns)\n",
    "    handle, = plt.plot(x, agent.rewards(), label=str(agent))\n",
    "    handles.append(handle)\n",
    "_ = plt.legend(handles=handles, loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
