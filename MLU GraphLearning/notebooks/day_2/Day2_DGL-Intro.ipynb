{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:32.338600Z",
     "start_time": "2021-01-03T21:15:32.021826Z"
    }
   },
   "source": [
    "![MLU Logo](../images/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "DGL at a Glance\n",
    "=========================\n",
    "\n",
    "The goal of this tutorial:\n",
    "\n",
    "- Understand how DGL enables computation on graph from a high level.\n",
    "- Train a simple graph neural network in DGL to classify nodes in a graph.\n",
    "\n",
    "At the end of this tutorial, we hope you get a brief feeling of how DGL works.\n",
    "\n",
    "*This tutorial assumes basic familiarity with pytorch.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T16:42:26.875398Z",
     "start_time": "2021-01-08T16:42:26.873514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dgl in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.6.0.post1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (1.5.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (2.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (1.19.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (2.25.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from networkx>=2.1->dgl) (4.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dgl\n",
    "!pip install torch\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial problem description\n",
    "----------------------------\n",
    "\n",
    "The tutorial is based on the \"Zachary's karate club\" problem. The karate club\n",
    "is a social network that includes 34 members and documents pairwise links\n",
    "between members who interact outside the club.  The club later divides into\n",
    "two communities led by the instructor (node 0) and the club president (node\n",
    "33). The network is visualized as follows with the color indicating the\n",
    "community:\n",
    "\n",
    "![](https://data.dgl.ai/tutorial/img/karate-club.png)\n",
    "\n",
    "\n",
    "The task is to predict which side (0 or 33) each member tends to join given\n",
    "the social network itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ./data and downloaded the files\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "course_ID = \"MLA-GML\"\n",
    "bucketname = \"mlu-courses-datalake\"\n",
    "\n",
    "gcn_filepath = course_ID + \"/data/gcn.png\"\n",
    "\n",
    "\n",
    "pathname = './data'\n",
    "s3 = boto3.resource('s3')\n",
    "if not os.path.exists(\"./data\"):\n",
    "    try:\n",
    "        os.makedirs(pathname)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % pathname)\n",
    "s3.Bucket(bucketname).download_file(gcn_filepath, \"./data/gcn.png\")\n",
    "print (\"Successfully created the directory %s and downloaded the files\" % pathname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Creating a graph in DGL\n",
    "-------------------------------\n",
    "Create the graph for Zachary's karate club as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:35.464709Z",
     "start_time": "2021-01-03T21:15:34.533998Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import dgl\n",
    "import numpy as np\n",
    "\n",
    "def build_karate_club_graph():\n",
    "    # All 78 edges are stored in two numpy arrays. One for source endpoints\n",
    "    # while the other for destination endpoints.\n",
    "    src = np.array([1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 6, 7, 7, 7, 7, 8, 8, 9, 10, 10,\n",
    "        10, 11, 12, 12, 13, 13, 13, 13, 16, 16, 17, 17, 19, 19, 21, 21,\n",
    "        25, 25, 27, 27, 27, 28, 29, 29, 30, 30, 31, 31, 31, 31, 32, 32,\n",
    "        32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33,\n",
    "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33])\n",
    "    dst = np.array([0, 0, 1, 0, 1, 2, 0, 0, 0, 4, 5, 0, 1, 2, 3, 0, 2, 2, 0, 4,\n",
    "        5, 0, 0, 3, 0, 1, 2, 3, 5, 6, 0, 1, 0, 1, 0, 1, 23, 24, 2, 23,\n",
    "        24, 2, 23, 26, 1, 8, 0, 24, 25, 28, 2, 8, 14, 15, 18, 20, 22, 23,\n",
    "        29, 30, 31, 8, 9, 13, 14, 15, 18, 19, 20, 22, 23, 26, 27, 28, 29, 30,\n",
    "        31, 32])\n",
    "    \n",
    "    # Edges are directional in DGL. Make them bi-directional.\n",
    "    u = np.concatenate([src, dst])\n",
    "    v = np.concatenate([dst, src])\n",
    "    \n",
    "    # Construct a DGLGraph\n",
    "    return dgl.DGLGraph((u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the number of nodes and edges in our newly constructed graph:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:36.968953Z",
     "start_time": "2021-01-03T21:15:36.964040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 34 nodes.\n",
      "We have 156 edges.\n",
      "CPU times: user 4.77 ms, sys: 64 µs, total: 4.84 ms\n",
      "Wall time: 3.48 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "G = build_karate_club_graph()\n",
    "print('We have %d nodes.' % G.number_of_nodes())\n",
    "print('We have %d edges.' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the graph by converting it to a [networkx](\n",
    "https://networkx.github.io/documentation/stable/) graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T16:37:52.678395Z",
     "start_time": "2021-01-08T16:37:52.673423Z"
    }
   },
   "source": [
    "Till now we only defined the dataset just like the week 1 notebook walkthrough.\n",
    "\n",
    "Let's move to the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Assign features to nodes or edges\n",
    "--------------------------------------------\n",
    "Graph neural networks associate features with nodes and edges for training.\n",
    "For our classification example, since there is no input feature, we assign each node\n",
    "with a learnable embedding vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:40.642455Z",
     "start_time": "2021-01-03T21:15:40.638718Z"
    }
   },
   "outputs": [],
   "source": [
    "# In DGL, you can add features for all nodes at once, using a feature tensor that\n",
    "# batches node features along the first dimension. The code below adds the learnable\n",
    "# embeddings for all nodes:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# X_a, X_b\n",
    "embed = nn.Embedding(34, 5)  # 34 nodes with embedding dim equal to 5\n",
    "G.ndata['feat'] = embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the node features to verify:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:41.783386Z",
     "start_time": "2021-01-03T21:15:41.778061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6903, -0.8352, -1.9306,  1.1125,  0.5706], grad_fn=<SelectBackward>)\n",
      "tensor([[ 0.6195,  1.1955, -0.9682, -1.1588,  1.0805],\n",
      "        [ 1.4310, -0.9853, -0.1343,  1.3694, -1.1998]],\n",
      "       grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "# print out node 2's input feature\n",
    "print(G.ndata['feat'][2])\n",
    "\n",
    "# print out node 10 and 11's input features\n",
    "print(G.ndata['feat'][[10, 11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define a Graph Convolutional Network (GCN)\n",
    "--------------------------------------------------\n",
    "To perform node classification, use the Graph Convolutional Network(GCN) developed by `Kipf and Welling <https://arxiv.org/abs/1609.02907>`. We recommend that you \n",
    "read the original paper for more details.\n",
    "\n",
    "- At layer $l$, each node $v_i^l$ carries a feature vector $h_i^l$.\n",
    "- Each layer of the GCN tries to aggregate the features from $u_i^{l}$ where\n",
    "  $u_i$'s are neighborhood nodes to $v$ into the next layer representation at\n",
    "  $v_i^{l+1}$. This is followed by an affine transformation with some\n",
    "  non-linearity.\n",
    "\n",
    "The above definition of GCN fits into a **message-passing** paradigm: Each\n",
    "node will update its own feature with information sent from neighboring\n",
    "nodes. \n",
    "\n",
    "As we saw in class, the following diagram can be used as a reference to understand GCNs - \n",
    "\n",
    "\n",
    "![GCN](./data/gcn.png)\n",
    "\n",
    "DGL has readily available implementations of popular Graph Neural Network layers under the `dgl.<backend>.nn` subpackage. The `dgl.nn.pytorch.GraphConv` module implements one Graph Convolutional layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:43.322587Z",
     "start_time": "2021-01-03T21:15:43.305153Z"
    }
   },
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a deeper GCN model that contains two GCN layers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:44.428467Z",
     "start_time": "2021-01-03T21:15:44.423615Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1.forward(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2.forward(g, h)\n",
    "        return h\n",
    "\n",
    "# The first layer transforms input features of size of 5 to a hidden size of 5.\n",
    "# The second layer transforms the hidden layer and produces output features of\n",
    "# size 2, corresponding to the two groups of the karate club.\n",
    "net = GCN(5, 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Data preparation and initialization\n",
    "-------------------------------------------\n",
    "\n",
    "We use learnable embeddings to initialize the node features. Since this is a\n",
    "semi-supervised setting, only the instructor (node 0) and the club president\n",
    "(node 33) are assigned labels. The implementation is available as follow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:15:45.783136Z",
     "start_time": "2021-01-03T21:15:45.780097Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = embed.weight\n",
    "labeled_nodes = torch.tensor([0, 33])  # only the instructor and the president nodes are labeled\n",
    "labels = torch.tensor([0, 1])  # their labels are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Train then visualize\n",
    "----------------------------\n",
    "\n",
    "The training loop is exactly the same as other PyTorch models.We (1) create an optimizer, (2) feed the inputs to the model, (3) calculate the loss and (4) use autograd to optimize the model.\n",
    "\n",
    "**NOTE** : Notice the use of random initializations (using `nn.Embedding`) here since we do not have access to any node features. In this case, there is no inductive bias stemming from the features themselves (random input will not effectively inform modifications of random neural network weights). The only source of inductive bias is the structure of the graph. We also modify the embeddings during training (look at the `itertools.chain` call below) - this does not really mean we can use the embeddings as representations for the nodes later, in fact, it promotes overfitting of the GCN parameters for this particular problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-03T21:15:48.716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.7043\n",
      "Epoch 1 | Loss: 0.6932\n",
      "Epoch 2 | Loss: 0.6821\n",
      "Epoch 3 | Loss: 0.6702\n",
      "Epoch 4 | Loss: 0.6574\n",
      "Epoch 5 | Loss: 0.6437\n",
      "Epoch 6 | Loss: 0.6287\n",
      "Epoch 7 | Loss: 0.6126\n",
      "Epoch 8 | Loss: 0.5954\n",
      "Epoch 9 | Loss: 0.5770\n",
      "Epoch 10 | Loss: 0.5579\n",
      "Epoch 11 | Loss: 0.5375\n",
      "Epoch 12 | Loss: 0.5163\n",
      "Epoch 13 | Loss: 0.4940\n",
      "Epoch 14 | Loss: 0.4696\n",
      "Epoch 15 | Loss: 0.4437\n",
      "Epoch 16 | Loss: 0.4169\n",
      "Epoch 17 | Loss: 0.3896\n",
      "Epoch 18 | Loss: 0.3620\n",
      "Epoch 19 | Loss: 0.3343\n",
      "Epoch 20 | Loss: 0.3070\n",
      "Epoch 21 | Loss: 0.2801\n",
      "Epoch 22 | Loss: 0.2540\n",
      "Epoch 23 | Loss: 0.2290\n",
      "Epoch 24 | Loss: 0.2049\n",
      "Epoch 25 | Loss: 0.1822\n",
      "Epoch 26 | Loss: 0.1611\n",
      "Epoch 27 | Loss: 0.1416\n",
      "Epoch 28 | Loss: 0.1239\n",
      "Epoch 29 | Loss: 0.1079\n",
      "Epoch 30 | Loss: 0.0935\n",
      "Epoch 31 | Loss: 0.0807\n",
      "Epoch 32 | Loss: 0.0695\n",
      "Epoch 33 | Loss: 0.0598\n",
      "Epoch 34 | Loss: 0.0513\n",
      "Epoch 35 | Loss: 0.0441\n",
      "Epoch 36 | Loss: 0.0379\n",
      "Epoch 37 | Loss: 0.0325\n",
      "Epoch 38 | Loss: 0.0280\n",
      "Epoch 39 | Loss: 0.0242\n",
      "Epoch 40 | Loss: 0.0210\n",
      "Epoch 41 | Loss: 0.0182\n",
      "Epoch 42 | Loss: 0.0159\n",
      "Epoch 43 | Loss: 0.0139\n",
      "Epoch 44 | Loss: 0.0122\n",
      "Epoch 45 | Loss: 0.0108\n",
      "Epoch 46 | Loss: 0.0096\n",
      "Epoch 47 | Loss: 0.0086\n",
      "Epoch 48 | Loss: 0.0077\n",
      "Epoch 49 | Loss: 0.0070\n",
      "CPU times: user 702 ms, sys: 64.2 ms, total: 766 ms\n",
      "Wall time: 418 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(net.parameters(), embed.parameters()), lr=0.01)\n",
    "\n",
    "all_logits = []\n",
    "for epoch in range(50):\n",
    "    logits = net(G, inputs)\n",
    "    # we save the logits for visualization later\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    # we only compute loss for labeled nodes\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a toy example, so it does not have a validation or test\n",
    "set. Instead, since the model produces an output feature of size 2 for each node, we can\n",
    "visualize by plotting the output feature in a 2D space.\n",
    "The following code animates the training process from initial guess\n",
    "(where the nodes are not classified correctly at all) to the end\n",
    "(where the nodes are linearly separable).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-03T21:15:15.128Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def draw(i):\n",
    "    cls1color = '#00FFFF'\n",
    "    cls2color = '#FF00FF'\n",
    "    pos = {}\n",
    "    colors = []\n",
    "    for v in range(34):\n",
    "        pos[v] = all_logits[i][v].numpy()\n",
    "        cls = pos[v].argmax()\n",
    "        colors.append(cls1color if cls else cls2color)\n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Epoch: %d' % i)\n",
    "    nx.draw_networkx(nx_G.to_undirected(), pos, node_color=colors,\n",
    "            with_labels=True, node_size=300, ax=ax)\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "nx_G = G.to_networkx()\n",
    "draw(0)  # draw the prediction of the first epoch\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://data.dgl.ai/tutorial/1_first/karate0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following animation shows how the model correctly predicts the community\n",
    "after a series of training epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://data.dgl.ai/tutorial/1_first/karate.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw your animation, uncomment and use the following code\n",
    "# from IPython.display import HTML\n",
    "# ani = animation.FuncAnimation(fig, draw, frames=len(all_logits), interval=200)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Exercise\n",
    "-------------------------------------------\n",
    "\n",
    "Define a GCN with 3 GraphConv layers (as opposed to 2 defined above). The size of the input (`in_feats`) and output (`num_classes`) remains the same. You can use any choice of hidden size for the intermediate layers. Use the ReLU non-linearity for each intermediate layer.\n",
    "\n",
    "Now try using this new GCN in the training loop above and see if there are any changes in the loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    FSTYPE LABEL UUID                                 MOUNTPOINT\r\n",
      "xvda                                                      \r\n",
      "└─xvda1 ext4   /     c41aa7cf-1eb0-4e70-9127-dedd83afc209 /\r\n",
      "xvdf    ext4         104d5b6d-b0ad-4f67-b466-704f7c195caa /home/ec2-user/SageMak\r\n"
     ]
    }
   ],
   "source": [
    "!lsblk -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
