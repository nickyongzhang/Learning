{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The final project deals with how to train a GNN model for node classification on the Amazon Co-purchase Network provided by OGB.\n",
    "\n",
    "The dataset contains 2.4 million nodes and 61 million edges, hence the full graph will not fit in a single GPU.\n",
    "\n",
    "The purpose of this project is mainly to show you an end-to-end working pipeline of training on a large graph, which you could potentially use in your own project. There are a **lot** of new concepts introduced in this notebook which is why we want to encourage you to spend time understanding the notebook first and start by making minor changes. Time permitting, you can go ahead and try to implement new layer architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By the end of this project you will learn how to train a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl-cu101\n",
      "  Downloading dgl_cu101-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (36.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.2 MB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from dgl-cu101) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from dgl-cu101) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from dgl-cu101) (1.19.5)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from dgl-cu101) (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from networkx>=2.1->dgl-cu101) (5.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (2.0.7)\n",
      "Installing collected packages: dgl-cu101\n",
      "Successfully installed dgl-cu101-0.6.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████▍                   | 322.0 MB 108.4 MB/s eta 0:00:05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |█████████████████████████▊      | 666.9 MB 111.3 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 831.4 MB 6.0 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.9.0) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.9.0) (3.10.0.2)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dgl-cu101\n",
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the OGB Products Dataset\n",
    "\n",
    "The [ogbn-products](https://ogb.stanford.edu/docs/nodeprop/#ogbn-products) dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. Node features are generated by extracting bag-of-words features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n",
    "\n",
    "**Prediction task** : The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels.\n",
    "\n",
    "**Dataset splitting** : Instead of randomly assigning 90% of the nodes for training and 10% of the nodes for testing (without use of a validation set), the sales ranking (popularity) is used to split nodes into training/validation/test sets. Specifically, the products are sorted according to their sales ranking and the top 8% are used for training, next top 2% for validation, and the rest for testing. This is a more challenging splitting procedure that closely matches the real-world application where labels are first assigned to important nodes in the network and ML models are subsequently used to make predictions on less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 1 : Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "course_ID = \"MLA-GML\"\n",
    "bucket_name = \"mlu-courses-datalake\"\n",
    "\n",
    "\n",
    "remote_dir_name = course_ID + \"/data/final_project/\"\n",
    "local_dir_name = \"dataset/\"\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = s3_resource.Bucket(bucket_name) \n",
    "for obj in bucket.objects.filter(Prefix = remote_dir_name):\n",
    "    if not os.path.exists(os.path.dirname(obj.key)):\n",
    "        os.makedirs(os.path.dirname(obj.key))\n",
    "    \n",
    "    try:\n",
    "        bucket.download_file(obj.key, obj.key)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The original dataset contains a lot of files. We will stick to only using these files:\n",
    "\n",
    "* `edge.csv` (source-destination pairs)\n",
    "* `node-feat.csv` (node features)\n",
    "* `node-label.csv` (node labels)\n",
    "* `train.csv` (node IDs in the training set)\n",
    "* `valid.csv` (node IDs in the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "edges = pd.read_csv('MLA-GML/data/final_project/edge.csv.gz', header=None).values\n",
    "node_features = pd.read_csv('MLA-GML/data/final_project/node-feat.csv.gz', header=None).values\n",
    "node_labels = pd.read_csv('MLA-GML/data/final_project/node-label.csv.gz', header=None).values[:, 0]\n",
    "\n",
    "# pd.read_csv yields a DataFrame with one column, so we make them one-dimensional arrays.\n",
    "train_nids = pd.read_csv('MLA-GML/data/final_project/train.csv.gz', header=None).values[:, 0]\n",
    "valid_nids = pd.read_csv('MLA-GML/data/final_project/valid.csv.gz', header=None).values[:, 0]\n",
    "test_nids = pd.read_csv('MLA-GML/data/final_project/test.csv.gz', header=None).values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2213091,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading Node IDs into DGL\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> The node IDs should be consecutive integers from 0 to the number of nodes minus 1.  If your node ID is not consecutive or not starting from 0 (e.g., starting from 100000), you need to relabel them yourself.  The <code>astype</code> method in pandas DataFrame can conveniently relabel the IDs by converting the type to <code>\"category\"</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2 : Construct DGL Graph\n",
    "\n",
    "We construct the graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "graph = dgl.graph((edges[:, 0], edges[:, 1]))\n",
    "graph = dgl.add_self_loop(graph)\n",
    "node_features = torch.FloatTensor(node_features)\n",
    "node_labels = torch.LongTensor(node_labels)\n",
    "\n",
    "# Save the graph, features and training-validation-test split for use for future tutorials.\n",
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump((graph, node_features, node_labels, train_nids, valid_nids), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load the graph back from the file we saved\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "with open('data.pkl', 'rb') as f:\n",
    "#     graph, node_features, node_labels, train_nids, valid_nids, test_nids = pickle.load(f)\n",
    "    graph, node_features, node_labels, train_nids, valid_nids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2449029, num_edges=64308169,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see the size of the graph, features, and labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph\n",
      "Graph(num_nodes=2449029, num_edges=64308169,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "Shape of node features: torch.Size([2449029, 100])\n",
      "Shape of node labels: torch.Size([2449029])\n",
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "print('Graph')\n",
    "print(graph)\n",
    "print('Shape of node features:', node_features.shape)\n",
    "print('Shape of node labels:', node_labels.shape)\n",
    "\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3 : Define a Data Loader with Neighbor Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Message passing overview\n",
    "\n",
    "The formulation of message passing usually has the following form:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_v^{(l)} = \\rho^{(l)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(l-1)} : u \\in \\mathcal{N} \\left( v \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_v^{(l)} = \\phi^{(l)} \\left(\n",
    "    \\boldsymbol{h}_v^{(l-1)}, \\boldsymbol{a}_v^{(l)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "where $\\rho^{(l)}$ and $\\phi^{(l)}$ are parameterized functions, and $\\mathcal{N}(v)$ represents the set of predecessors (or equivalently *neighbors*) of $v$ on graph $\\mathcal{G}$:\n",
    "$$\n",
    "\\mathcal{N} \\left( v \\right) = \\left\\lbrace\n",
    "  s \\left( e \\right) : e \\in \\mathbb{E}, t \\left( e \\right) = v\n",
    "\\right\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MLA-GML-Content/notebooks/final_project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "For instance, to perform a message passing for updating the red node in the following graph:\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You need to aggregate the node features of its neighbors, shown as green nodes:\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's consider how multi-layer message passing works for computing the output of a single node.  In the following text, we refer to the nodes whose outputs GNN will compute as seed nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Multi-layer message passing \n",
    "\n",
    "Consider computing with a 2-layer GNN the output of the seed node 8, colored red, in the following graph:\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/seed.png)\n",
    "\n",
    "By the formulation:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_8^{(2)} = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(1)} : u \\in \\mathcal{N} \\left( 8 \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right) = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_4^{(1)}, \\boldsymbol{h}_5^{(1)},\n",
    "      \\boldsymbol{h}_7^{(1)}, \\boldsymbol{h}_{11}^{(1)}\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_8^{(2)} = \\phi^{(2)} \\left(\n",
    "    \\boldsymbol{h}_8^{(1)}, \\boldsymbol{a}_8^{(2)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can tell that, to compute $\\boldsymbol{h}_8^{(2)}$, we need messages from node 4, 5, 7, and 11 (colored green) along the edges visualized below.\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/3.png)\n",
    "\n",
    "The values of $\\boldsymbol{h}_\\cdot^{(1)}$ are the outputs from the first GNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To compute those values for the red and green nodes, we further need to perform message passing on the edges visualized below.\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Therefore, to compute the 2-layer GNN representation of the red node, we need the input features from the red node as well as the green and yellow nodes.  Note that we should take red node's neighbors again for this layer.\n",
    "\n",
    "You may notice that the procedure which determines computation dependency is in the reverse direction of message aggregation: you start from the layer closest to the output and work backward to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "* Computing representation for a small number of nodes still often requires input features of a significantly larger number of nodes.  \n",
    "\n",
    "* Taking all neighbors for message aggregation is often too costly since the nodes needed would easily cover a large portion of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neighbour sampling addresses this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neighbour Sampling Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neighbor sampling addresses this issue by selecting a random subset of the neighbors to perform aggregation.\n",
    "\n",
    "For example, to compute $\\boldsymbol{h}_8^{(1)}$, we can choose to sample 2 neighbors and aggregate.\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, to compute the red and green nodes' first layer representation, we can also do neighbor sampling that takes 2 neighbors for each node.  Note that we should take the red node's neighbors again for this layer.\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/6.png)\n",
    "\n",
    "You can see that this method could give us fewer nodes needed for input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Other graph sampling strategies\n",
    "* Neighborhood sampling (GraphSAGE)\n",
    "* Control-variate-based sampling (VRGCN)\n",
    "* Layer-wise sampling (FastGCN, LADIES)\n",
    "* Random-walk-based sampling (PinSage)\n",
    "* Subgraph sampling (ClusterGCN, GraphSAINT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining neighbor sampler and node data loader in DGL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches and performing neighbor sampling.\n",
    "\n",
    "For node classification, you can use\n",
    "* [`dgl.dataloading.NodeDataLoader`](https://docs.dgl.ai/en/0.6.x/api/python/dgl.dataloading.html#dgl.dataloading.pytorch.NodeDataLoader) for iterating over the dataset, and\n",
    "* [`dgl.dataloading.MultiLayerNeighborSampler`](https://docs.dgl.ai/en/0.6.x/api/python/dgl.dataloading.html#dgl.dataloading.neighbor.MultiLayerNeighborSampler) to generate computation dependencies of the nodes with neighbor sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's consider training a 3-layer GNN with neighbor sampling, and each node will gather message from 4 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can peek at the first item in the data loader we created and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([158124, 162988,  12835,  ...,  45071,  39915,  76617]), tensor([158124, 162988,  12835,  ..., 181952,  52808, 138705]), [Block(num_src_nodes=33317, num_dst_nodes=14939, num_edges=52863), Block(num_src_nodes=14939, num_dst_nodes=4411, num_edges=16293), Block(num_src_nodes=4411, num_dst_nodes=1024, num_edges=3882)]]\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 nodes' output we need 33317 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} nodes' output we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The variable `bipartites` has the message passing computation dependency for each layer.\n",
    "\n",
    "It is named suggestively, because it can be thought of as a **list** of bipartite graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So why does DGL return a list of *bipartite* graphs for training a *homogeneous* graph? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To distinguish between the source nodes sending the messages and the destination nodes being updated at each layer.\n",
    "\n",
    "Recall the sampled sub-graph from the example above:\n",
    "\n",
    "![Imgur](./MLA-GML/data/final_project/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first GNN layer outputs the representation of three nodes (two green nodes and one red node), but requires input from 7 nodes (the green nodes and red node, plus 4 yellow nodes).  \n",
    "\n",
    "A bipartite graph easily captures the computation dependency -\n",
    "\n",
    "![](./MLA-GML/data/final_project/bipartite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at each *bipartite* graph in `bipartites`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(num_src_nodes=33317, num_dst_nodes=14939, num_edges=52863)\n",
      "Block(num_src_nodes=14939, num_dst_nodes=4411, num_edges=16293)\n",
      "Block(num_src_nodes=4411, num_dst_nodes=1024, num_edges=3882)\n"
     ]
    }
   ],
   "source": [
    "for block in bipartites:\n",
    "    print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These represent the bipartites to be used at each layerof the graph unrolling.\n",
    "\n",
    "Minibatch training of GNNs usually involves message passing on such bipartite graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 4 : Main Project Task - Defining Model Architecture\n",
    "\n",
    "We are training a GraphSage GNN model that was previously introduced.\n",
    "\n",
    "An example model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "# TODO : Try out different layer implementations here. For eg. You could try replacing 'mean' with \n",
    "#        different aggregation strategies, change the number of layers etc. An advanced implementation\n",
    "#        could also include implementing a custom layer like we did in the Day 3 notebook.\n",
    "#        It is advisable to keep the rest of the code structure the same. \n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(dglnn.GATConv(in_feats, n_hidden, num_heads=3, attn_drop=0.05, activation=F.relu))\n",
    "            in_feats = n_hidden * 3\n",
    "            \n",
    "        self.classify = nn.Linear(n_hidden*3, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        # Iterate over the layers\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            x = x.flatten(1)\n",
    "        x = self.classify(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can see that here we are iterating over the pairs of NN module layer and bipartite graphs generated by the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 5 : Defining the Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Note the .cuda() which will send the model to the GPU\n",
    "model = GAT(num_features, 128, num_classes, 3).cuda()\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (layers): ModuleList(\n",
       "    (0): GATConv(\n",
       "      (fc): Linear(in_features=100, out_features=384, bias=False)\n",
       "      (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "      (attn_drop): Dropout(p=0.05, inplace=False)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): GATConv(\n",
       "      (fc): Linear(in_features=384, out_features=384, bias=False)\n",
       "      (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "      (attn_drop): Dropout(p=0.05, inplace=False)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (2): GATConv(\n",
       "      (fc): Linear(in_features=384, out_features=384, bias=False)\n",
       "      (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "      (attn_drop): Dropout(p=0.05, inplace=False)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (classify): Linear(in_features=384, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataloader for validation\n",
    "\n",
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.47it/s, loss=1.104, acc=0.857]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.59it/s]\n",
      "  1%|          | 2/193 [00:00<00:16, 11.87it/s, loss=0.700, acc=0.815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8413396739821478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.63it/s, loss=0.616, acc=0.714]\n",
      "100%|██████████| 39/39 [00:02<00:00, 13.26it/s]\n",
      "  1%|          | 2/193 [00:00<00:16, 11.69it/s, loss=0.754, acc=0.793]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.8445693360120031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.39it/s, loss=0.763, acc=0.857]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.60it/s]\n",
      "  1%|          | 2/193 [00:00<00:15, 12.10it/s, loss=0.553, acc=0.847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.8554026905373445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.32it/s, loss=0.362, acc=0.857]\n",
      "100%|██████████| 39/39 [00:02<00:00, 13.06it/s]\n",
      "  1%|          | 2/193 [00:00<00:16, 11.34it/s, loss=0.549, acc=0.843]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.8534445489916842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.41it/s, loss=1.160, acc=0.714]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.68it/s]\n",
      "  1%|          | 2/193 [00:00<00:16, 11.79it/s, loss=0.669, acc=0.823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.8407293441497342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.38it/s, loss=0.476, acc=0.857]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.43it/s]\n",
      "  1%|          | 2/193 [00:00<00:16, 11.92it/s, loss=0.514, acc=0.862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.8604887724741246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.60it/s, loss=0.044, acc=1.000]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.72it/s]\n",
      "  1%|          | 2/193 [00:00<00:15, 12.08it/s, loss=0.417, acc=0.887]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.8690333901279149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.18it/s, loss=0.130, acc=1.000]\n",
      "100%|██████████| 39/39 [00:02<00:00, 13.36it/s]\n",
      "  1%|          | 2/193 [00:00<00:17, 11.20it/s, loss=0.436, acc=0.875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.8673549830887776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.55it/s, loss=0.427, acc=0.857]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.71it/s]\n",
      "  1%|          | 2/193 [00:00<00:16, 11.83it/s, loss=0.472, acc=0.872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.86521882867533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:15<00:00, 12.43it/s, loss=0.063, acc=1.000]\n",
      "100%|██████████| 39/39 [00:03<00:00, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.871067822902627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    # set the model to train mode as required by PyTorch\n",
    "    model.train()\n",
    "    \n",
    "    # tqdm simply draws progress bars\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        \n",
    "        # Iterate over the training data (nodes)\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            \n",
    "            # Send each of the layers of the graph structures to be used, to the GPU\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            # Send the node features & labels to the GPU\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            \n",
    "            # Make predictions based on the current initializations\n",
    "            predictions = model(bipartites, inputs)\n",
    "            \n",
    "            # Calculate the classification loss between the predictions and labels\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            \n",
    "            # Common steps used within PyTorch\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            # Calculate the accuracy of the predicted labels w.r.t ground truth labels\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            # Just a tqdm thing\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "    \n",
    "    # Switch to eval mode for checking the validation set\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    # Same loop as above, albeit a bit compressed\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels.append(node_labels[output_nodes].numpy())\n",
    "            predictions.append(model(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "            \n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        \n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 6 : Generate a submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "Usually for offline inference it is desirable to aggregate over the entire neighborhood to eliminate randomness introduced by neighbor sampling.  However, using the same methodology in training is not efficient, because there will be a lot of redundant computation.  Moreover, simply doing neighbor sampling by taking all neighbors will often exhaust GPU memory because the number of nodes required for input features may be too large to fit into GPU memory.\n",
    "\n",
    "Instead, you need to compute the representations layer by layer: you first compute the output of the first GNN layer for all nodes, then you compute the output of second GNN layer for all nodes using the first GNN layer's output as input, etc.  \n",
    "\n",
    "This gives us a different algorithm from what is being used in training.  During training we have an outer loop that iterates over the nodes, and an inner loop that iterates over the layers.  In contrast, during inference we have an outer loop that iterates over the layers, and an inner loop that iterates over the nodes.\n",
    "\n",
    "If you do not care about randomness too much (e.g., during model selection in validation), you can still use the `dgl.dataloading.MultiLayerNeighborSampler` and `dgl.dataloading.NodeDataLoader` to do offline inference, since it is usually faster for evaluating a small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Double click on this cell and add a `t` to complete the word `dataset` for seeing the actual animation](./MLA-GML/data/final_project/anim.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Make sure the model is in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            # This buffer will maintain the node representations at each level of the graph unrolling\n",
    "            output_features = torch.zeros(\n",
    "                graph.number_of_nodes(), model.n_hidden * 3\n",
    "            )\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                # bipartites[0] bc bipartites is a list with a single element that contains one graph unroll step\n",
    "                # with ALL neighbors for the nodes in that batch\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                x = x.flatten(1)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "          \n",
    "        output_features = torch.zeros(\n",
    "                graph.number_of_nodes(), model.n_classes\n",
    "            )\n",
    "        num_of_batch = graph.number_of_nodes()//batch_size\n",
    "        for i in range(num_of_batch+1):\n",
    "            x = input_features[i*batch_size:(i+1)*batch_size,:].to(torch.device('cuda'))\n",
    "            x = model.classify(x)\n",
    "            output_features[i*batch_size:(i+1)*batch_size,:] = x.cpu()\n",
    "            \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [01:18<00:00,  3.82it/s]\n",
      "100%|██████████| 299/299 [01:08<00:00,  4.39it/s]\n",
      "100%|██████████| 299/299 [01:06<00:00,  4.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7570822889795313"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "all_predictions = inference(model, graph, node_features, 8192)\n",
    "\n",
    "test_predictions = all_predictions[test_nids].argmax(1)\n",
    "sklearn.metrics.accuracy_score(node_labels[test_nids], test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7451376431445024"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = all_predictions[test_nids[:663927]].argmax(1)\n",
    "sklearn.metrics.accuracy_score(node_labels[test_nids[:663927]], test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_to_file(filename, ids, predictions):\n",
    "    with open(filename,'w') as out:\n",
    "        csv_out = csv.writer(out)\n",
    "        csv_out.writerow(['ID','Label'])\n",
    "        for index, label in zip(ids, predictions):\n",
    "            csv_out.writerow((index, label))\n",
    "test_predictions = all_predictions[test_nids].argmax(1)\n",
    "write_to_file('submission.csv', range(len(test_predictions)), test_predictions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GNN with neighbor sampling on a large dataset that cannot fit into GPU at once.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "Running the last cell above will write your predictions out to a file called submission.csv in your current folder. A sample submission file has been included for you to make a test submission, and then you can improve upon it with more experimentation. Download it to your local machine and upload it to https://leaderboard.corp.amazon.com/tasks/703/submit\n",
    "\n",
    "A single submission is enough to qualify you for completion.\n",
    "\n",
    "Experiment with various models of GNNs and see if you can go above the performance of the simple network defined above.\n",
    "\n",
    "\n",
    "**NOTE** : Since the test data is public, you could, in theory, just make a submission using the ground truth labels and get a 1.0 on the leaderboard. This is not helping you learn anything and although it qualifies as a completion it will likely automatically disqualify you from being considered a top submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
