{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Multiprocessing dataset computing\n",
    "\n",
    "Dask allows managing large datasets through many data structures such as Dask DataFrame and Dask Bags that we already covered on the slides. The following paragraphs are left to you if you want to go more in detail on their functioning.\n",
    "\n",
    "### Dask DataFrame\n",
    "The ```dask.dataframe``` module implements a blocked parallel DataFrame object that mimics a large subset of the Pandas DataFrame. One Dask DataFrame is composed by of many pandas DataFrames. Each Pandas dataframe is a \"*partition*\" of the distributed dataframe. One operation on a Dask DataFrame triggers the same pandas operations on the constituent pandas DataFrames keeping into account the parallelism and memory constraints. \n",
    "\n",
    "In light of this, Dask DataFrame is important for two reasons mainly:\n",
    "\n",
    "+ It is familiar to Pandas users\n",
    "+ The partitioning approach is important for efficient queries\n",
    "\n",
    "Is it easy to see that, from a Spark perspective, those DataFrames are basically a lighter and pythonic version of the Spark DataFrames.\\\n",
    "As Pandas, Dask DataFrame supports a large set of input data formats as:\n",
    "+ CSV\n",
    "+ Parquet\n",
    "+ Avro\n",
    "+ XML\n",
    "+ Excel\n",
    "+ ...\n",
    "\n",
    "Even though these DataFrames are powerful, *Pandas is more mature and fully featured*. If your data fits in memory, then you should use Pandas. One of the main reasons is that not all the Pandas API have been implemented in Dask yet.\n",
    "\n",
    "\n",
    "### Dask Bag \n",
    "Alongside the DataFrames, Dask offers another data collection structure: The Bag data structure. This data structure implements operations like `map`, `filter`, `groupby`, and aggregations on collections of Python objects. It does this in parallel and in small memory using Python iterators. It is similar to a parallel version of itertools, but it can be also considered a more lighter and pythonic version of the *Spark RDD*.\n",
    "\n",
    "Dask Bags are often used to do simple preprocessing on log files, JSON records, or other user-defined Python objects.\\\n",
    "Basically, Bags is a good substitutes to the DataFrames when the data are non or partially structured.\n",
    "\n",
    "In our daily use cases, dask Bag structures are an optimal tool to manage, extract and work with structured or non-structured text files, where we can have hundreds of gigabytes of data that must be elaborated. Some examples are:\n",
    "+ WBQA traces\n",
    "+ Jsonl datasets\n",
    "+ WBQA logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: a real use case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Hands-on session is designed to be really close to our daily use-cases. To this end, we prepared some data from Wikipedia dumps. The data must be downloaded from [URL](https://drive.corp.amazon.com/personal/lauivano/Workshop-dask-data). Once unzipped the downloaded file will find two data folders:\n",
    "+ data_full: for who is using an EC2 instance. It contains 100 parquet files for a total of 1.2GB\n",
    "+ data_small: for who is using its own laptop. It contains 50 parquet files for a total of ~300Mb\n",
    "\n",
    "Each folders contains a set o files, each file contains a collection of documents, and each document is composed by three fields:\n",
    "+ id: UUID\n",
    "+ title: text\n",
    "+ content: text\n",
    "\n",
    "Given these files, we want to:\n",
    "+ count the number of words in each document\n",
    "+ get the mean and the stdev of the number of words with respect to all documents\n",
    "+ count the number of sentences for each document\n",
    "+ get the document with the max number of sentences\n",
    "+ count how many times the word \"amazon\" occurs in those documents\n",
    "+ take the top 10 documents that contain the larger number of \"amazon\" occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a cluster\n",
    "To instantiate a cluster in our machines we have to use the `LocalCluster` object as previously seen in the slide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS_NUMBER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(name=\"workshop_cluster\", n_workers=4)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT**: The client object now is our entry point to the cluster. Anyway, DataFrames and Bags do not need to directly use this object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the provieded files\n",
    "In this step we read the files through the `dask.dataframe` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you have a good computing resource (EC2) please use the 'data_full' folder instead of 'data_small'\n",
    "df = dd.read_parquet('data_small').persist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens if we tray to look into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the columns seem to be empty. This is because, Dask is *lazy*.\\\n",
    "When we work in a distributed environment, we have to keep in mind that the computations are triggered only when they are necessary. Let's see what happens if we call the ```head``` method of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the computation was longer than before. This is because the `head` method triggered the dataset reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Count the number of words of each document\n",
    "\n",
    "In this step we use the `NLTK` library to tokenize the text. After that, we count how many words each document contains\n",
    "\n",
    "#### Step 3a - Document tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenization_function(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(doc_words = df.text.apply(tokenization_function, meta=list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the computational graph and see what is happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid to run this cell if you have not installed the graphviz package\n",
    "# double click on the below image to zoom\n",
    "df.visualize(ranking=\"LR\", filename='3a-graph.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computational graph represents operations executed in parallel for the dataset.\n",
    "\n",
    "However, sometimes it is not useful to do this in a lazy way, since each time you require the field `doc_words` you need to recompute it unless you use the `compute()` method.\n",
    "\n",
    "Having said that, the natural thought could be to use the `compute` method to execute the operation and reuse the results without recomputing them.\\\n",
    "That's not true. \\\n",
    "We can use the `persist` method. This method allows you to fix in the workers memory the dataset and its partial operations or new fields avoiding its recomputation. Moreover, Dask has a sort of caching mechanism, so even if you don't use the aforementioned method, some data are retrieved from the cache rather than recomputing them.\n",
    "Let's see the difference in terms of computation of running `head` method before and after the `persist`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3b - Document word number count\n",
    "After words tokenization we can compute, for each document, the number of words it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(text_words):\n",
    "    return len(text_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.assign(doc_words_number=df.doc_words.apply(word_counter, meta=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the mean and the stdev of the number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute the mean and the standard deviation of the number of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean = df.doc_words_number.mean().compute()\n",
    "stdev = df.doc_words_number.std().compute()\n",
    "print(f'Mean of number of words is {round(mean, 2)} +/- {round(stdev,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step performs two computations that are quite similar and that share some partial results. \\\n",
    "As seen in the presentation there is a way to optimize these computations?\\\n",
    "Yes: we can merge those computations making Dask able to generate a unique graph and reusing some common partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean, stdev = dask.compute(df.doc_words_number.mean(), df.doc_words_number.std())\n",
    "print(f'Mean of number of words is {round(mean, 2)} +/- {round(stdev,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Count the number of sentences of each document\n",
    "\n",
    "The goal of this point is to use the NLTK library to tokenize the sentences of the text and to count how many sentences each document contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5a - Document sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def sentence_tokenization_function(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(doc_sentences = df.text.apply(..., meta=list)) #complete the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's persist into the cluster memory the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5b - Document sentences number count\n",
    "\n",
    "Ok, now we are ready to count the number of sentences per document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_counter(text_sentences):\n",
    "    return len(text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(doc_sentences_number= ... ) #complete the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Get the document with the larger number of phrases \n",
    "Once finished the previous step, we can extract the document with larger number of sentences in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.doc_sentences_number == df.doc_sentences_number.max()].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Count how many time the word \"amazon\" occur in those documents\n",
    "\n",
    "Let's move on and try to count the occurrences of the word \"*amazon*\" for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_amazon_word(text):\n",
    "    return text.lower().count('amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(occurencies_of_amazon= ... )#complete the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Take the top 10 documents that contain the larger number of \"amazon\" occurrences\n",
    "\n",
    "At first let's filter the data in orther to keep only the document that contains at least one occurrence of our target word.\n",
    "\n",
    "Then we can sort the dataset by ```occurencies_of_amazon``` column, and then extract the top 10 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_that_contains_word_amazon = df.loc[df.occurencies_of_amazon != 0].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_that_contains_word_amazon = text_that_contains_word_amazon.sort_values('occurencies_of_amazon').reset_index(drop=True)\n",
    "text_that_contains_word_amazon.tail(10)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we finished all we planned is a easy way!\n",
    "Now, remember to close the cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to joined this session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
